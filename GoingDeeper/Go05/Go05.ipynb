{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a7f2d7e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import tensorflow \n",
    "import nltk\n",
    "import gensim\n",
    "import re\n",
    "from konlpy.tag import Mecab\n",
    "from sklearn.model_selection import train_test_split\n",
    "import tensorflow as tf\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "6d002169",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3.8.3\n"
     ]
    }
   ],
   "source": [
    "print(gensim.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "aa590a76",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('ChatbotData.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "89f6aa52",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Q</th>\n",
       "      <th>A</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>12시 땡!</td>\n",
       "      <td>하루가 또 가네요.</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1지망 학교 떨어졌어</td>\n",
       "      <td>위로해 드립니다.</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3박4일 놀러가고 싶다</td>\n",
       "      <td>여행은 언제나 좋죠.</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3박4일 정도 놀러가고 싶다</td>\n",
       "      <td>여행은 언제나 좋죠.</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>PPL 심하네</td>\n",
       "      <td>눈살이 찌푸려지죠.</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11818</th>\n",
       "      <td>훔쳐보는 것도 눈치 보임.</td>\n",
       "      <td>티가 나니까 눈치가 보이는 거죠!</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11819</th>\n",
       "      <td>훔쳐보는 것도 눈치 보임.</td>\n",
       "      <td>훔쳐보는 거 티나나봐요.</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11820</th>\n",
       "      <td>흑기사 해주는 짝남.</td>\n",
       "      <td>설렜겠어요.</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11821</th>\n",
       "      <td>힘든 연애 좋은 연애라는게 무슨 차이일까?</td>\n",
       "      <td>잘 헤어질 수 있는 사이 여부인 거 같아요.</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11822</th>\n",
       "      <td>힘들어서 결혼할까봐</td>\n",
       "      <td>도피성 결혼은 하지 않길 바라요.</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>11823 rows × 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                             Q                         A  label\n",
       "0                       12시 땡!                하루가 또 가네요.      0\n",
       "1                  1지망 학교 떨어졌어                 위로해 드립니다.      0\n",
       "2                 3박4일 놀러가고 싶다               여행은 언제나 좋죠.      0\n",
       "3              3박4일 정도 놀러가고 싶다               여행은 언제나 좋죠.      0\n",
       "4                      PPL 심하네                눈살이 찌푸려지죠.      0\n",
       "...                        ...                       ...    ...\n",
       "11818           훔쳐보는 것도 눈치 보임.        티가 나니까 눈치가 보이는 거죠!      2\n",
       "11819           훔쳐보는 것도 눈치 보임.             훔쳐보는 거 티나나봐요.      2\n",
       "11820              흑기사 해주는 짝남.                    설렜겠어요.      2\n",
       "11821  힘든 연애 좋은 연애라는게 무슨 차이일까?  잘 헤어질 수 있는 사이 여부인 거 같아요.      2\n",
       "11822               힘들어서 결혼할까봐        도피성 결혼은 하지 않길 바라요.      2\n",
       "\n",
       "[11823 rows x 3 columns]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "17fec1df",
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_sentence(sentence):\n",
    "    sentence = sentence.lower() # 소문자로 변환\n",
    "    sentence = re.sub(r'[^a-z0-9가-힣!,.?]', ' ', sentence) # 영어, 숫자, 한글, !,.? 포함\n",
    "    return sentence\n",
    "\n",
    "\n",
    "def build_corpus(Q, A, token_len=None):\n",
    "    mecab  = Mecab()\n",
    "    temp = pd.concat([Q,A], axis=1)\n",
    "    \n",
    "    # 중복제거\n",
    "    temp = temp.drop_duplicates('Q') # Q에서 중복있으면 삭제\n",
    "    temp = temp.drop_duplicates('A') # A에서 중복있으면 삭제\n",
    "    \n",
    "    # preprocess\n",
    "    temp['Q'] = temp['Q'].apply(preprocess_sentence)\n",
    "    temp['A'] = temp['A'].apply(preprocess_sentence)\n",
    "    \n",
    "    #mecab 사용 형태소 추출\n",
    "    temp['Q'] = temp['Q'].apply(lambda x: mecab.pos(x))\n",
    "    temp['A'] = temp['A'].apply(lambda x: mecab.pos(x))\n",
    "#     temp['Q'] = temp['Q'].apply(lambda x: mecab.morphs(x))\n",
    "#     temp['A'] = temp['A'].apply(lambda x: mecab.morphs(x))\n",
    "    \n",
    "    if token_len is not None:\n",
    "        temp = temp[(temp['Q'].apply(len) < token_len) & (temp['A'].apply(len) < token_len)]\n",
    "        \n",
    "    return temp['Q'].values, temp['A'].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "37c22ce9",
   "metadata": {},
   "outputs": [],
   "source": [
    "que_corpus, ans_corpus  = build_corpus(df['Q'], df['A'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "c54c300a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([list([('12', 'SN'), ('시', 'NNBC'), ('땡', 'MAG'), ('!', 'SF')]),\n",
       "       list([('1', 'SN'), ('지망', 'NNG'), ('학교', 'NNG'), ('떨어졌', 'VV+EP'), ('어', 'EC')]),\n",
       "       list([('3', 'SN'), ('박', 'NNBC'), ('4', 'SN'), ('일', 'NNBC'), ('놀', 'VV'), ('러', 'EC'), ('가', 'VX'), ('고', 'EC'), ('싶', 'VX'), ('다', 'EC')]),\n",
       "       ...,\n",
       "       list([('훔쳐', 'VV+EC'), ('보', 'VX'), ('는', 'ETM'), ('것', 'NNB'), ('도', 'JX'), ('눈치', 'NNG'), ('보임', 'VV+ETN'), ('.', 'SF')]),\n",
       "       list([('흑기사', 'NNG'), ('해', 'VV+EC'), ('주', 'VX'), ('는', 'ETM'), ('짝', 'VA'), ('남', 'EF'), ('.', 'SF')]),\n",
       "       list([('힘든', 'VA+ETM'), ('연애', 'NNG'), ('좋', 'VA'), ('은', 'ETM'), ('연애', 'NNG'), ('라는', 'VCP+ETM'), ('게', 'NNB+JKS'), ('무슨', 'MM'), ('차이', 'NNG'), ('일까', 'VCP+EF'), ('?', 'SF')])],\n",
       "      dtype=object)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "que_corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "f630ed50",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([list([('하루', 'NNG'), ('가', 'JKS'), ('또', 'MAG'), ('가', 'VV'), ('네요', 'EF'), ('.', 'SF')]),\n",
       "       list([('위로', 'NNG'), ('해', 'XSV+EC'), ('드립니다', 'VX+EF'), ('.', 'SF')]),\n",
       "       list([('여행', 'NNG'), ('은', 'JX'), ('언제나', 'MAG'), ('좋', 'VA'), ('죠', 'EF'), ('.', 'SF')]),\n",
       "       ...,\n",
       "       list([('티', 'NNG'), ('가', 'JKS'), ('나', 'VV'), ('니까', 'EC'), ('눈치', 'NNG'), ('가', 'JKS'), ('보이', 'VV'), ('는', 'ETM'), ('거', 'NNB'), ('죠', 'VCP+EF'), ('!', 'SF')]),\n",
       "       list([('설렜', 'VV+EP'), ('겠', 'EP'), ('어요', 'EF'), ('.', 'SF')]),\n",
       "       list([('잘', 'MAG'), ('헤어질', 'VV+ETM'), ('수', 'NNB'), ('있', 'VV'), ('는', 'ETM'), ('사이', 'NNG'), ('여부', 'NNG'), ('인', 'VCP+ETM'), ('거', 'NNB'), ('같', 'VA'), ('아요', 'EF'), ('.', 'SF')])],\n",
       "      dtype=object)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ans_corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "90ec0640",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(7731, 7731)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(que_corpus), len(ans_corpus)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "e1a4f506",
   "metadata": {},
   "outputs": [],
   "source": [
    "cut_len = int(len(que_corpus) * 0.8)\n",
    "train_que = que_corpus[:cut_len]\n",
    "train_ans = ans_corpus[:cut_len]\n",
    "valid_que = que_corpus[cut_len:]\n",
    "valid_ans = ans_corpus[cut_len:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "342c603c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(6184, 6184, 1547, 1547)"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(train_que), len(train_ans), len(valid_que), len(valid_ans)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "e10c55cf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Word2Vec(vocab=30185, size=200, alpha=0.025)\n"
     ]
    }
   ],
   "source": [
    "wv_model = gensim.models.Word2Vec.load('ko.bin')\n",
    "# 모델이 정상적으로 로드되었는지 확인\n",
    "print(wv_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "8493ece7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "30185\n",
      "200\n"
     ]
    }
   ],
   "source": [
    "VOCAB_SIZE = len(wv_model.wv.vocab)\n",
    "print(VOCAB_SIZE)\n",
    "print(wv_model.vector_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "7d3f8341",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('낱말', 0.8457916975021362),\n",
       " ('용어', 0.7469839453697205),\n",
       " ('어휘', 0.7309367656707764)]"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wv_model.wv.most_similar('단어', topn=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "c914afb1",
   "metadata": {},
   "outputs": [],
   "source": [
    "mecab  = Mecab()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "4bc4005e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import itertools\n",
    "def lexical_sub(old_src, wv, topn, similarity_threshold):\n",
    "    mecab  = Mecab()\n",
    "#     pos_tags=['NNP', 'VV', 'NP']  # 일반명사, 고유명사, 의존명사, 대명사\n",
    "    pos_tags=['NNG']  # 일반명사, 고유명사, 의존명사, 대명사\n",
    "    \n",
    "    # 원문 문자열\n",
    "    original = [tok for tok, _ in old_src]\n",
    "\n",
    "    # 후보 단어 위치 + 유사어 리스트 추출\n",
    "    candidate_subs = []\n",
    "    for i, (tok, pos) in enumerate(old_src):\n",
    "#         print(i, (tok, pos))\n",
    "        if pos in pos_tags and tok in wv:\n",
    "#             print(tok)\n",
    "            try:\n",
    "                similar_list = wv.wv.most_similar(tok, topn=topn)\n",
    "                filtered = []\n",
    "                for w, sim in similar_list:\n",
    "                    temp_pos = mecab.pos(w)\n",
    "#                     print(pos, temp_pos, tok, temp_pos[0][0])\n",
    "                    if sim >= similarity_threshold and pos == temp_pos[0][1]:\n",
    "                        filtered.append(w)\n",
    "#                 filtered = [w for w, sim in similar_list if sim >= similarity_threshold]\n",
    "                if filtered:\n",
    "                    candidate_subs.append((i, filtered))\n",
    "            except:\n",
    "                continue\n",
    "\n",
    "    # 후보가 없으면 원문만 반환\n",
    "    if not candidate_subs:\n",
    "        return [original]\n",
    "\n",
    "    # 각 후보 위치마다 가능한 치환 리스트를 구성\n",
    "    # 예: [(3, ['A', 'B']), (5, ['X', 'Y'])] → 치환 조합: [ (A,X), (A,Y), (B,X), (B,Y) ]\n",
    "    all_versions = []\n",
    "\n",
    "    positions, replacements = zip(*candidate_subs)\n",
    "    for combo in itertools.product(*replacements):  # 모든 조합 생성\n",
    "        new_sent = original.copy()\n",
    "        for idx, rep in zip(positions, combo):\n",
    "            new_sent[idx] = rep\n",
    "        all_versions.append(new_sent)\n",
    "\n",
    "    # 원문 포함\n",
    "    all_versions.insert(0, original)\n",
    "\n",
    "    return all_versions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "baa57213",
   "metadata": {},
   "outputs": [],
   "source": [
    "from itertools import product\n",
    "def augment_pairwise_cross_product_df(enc_src, dec_tgt, wv, topn=3, similarity_threshold=0.6):\n",
    "    \"\"\"\n",
    "    enc_src, dec_tgt: 형태소+POS 리스트 (예: mecab.pos 결과들)\n",
    "    반환값: 증강된 (que, ans) 문장쌍을 담은 DataFrame\n",
    "    \"\"\"\n",
    "    rows = []\n",
    "\n",
    "    for enc_tokens, dec_tokens in zip(enc_src, dec_tgt):\n",
    "        enc_augments = lexical_sub(enc_tokens, wv, topn, similarity_threshold)\n",
    "        dec_augments = lexical_sub(dec_tokens, wv, topn, similarity_threshold)\n",
    "\n",
    "        for enc_version, dec_version in product(enc_augments, dec_augments):\n",
    "            rows.append({\n",
    "                'que': ' '.join(enc_version),\n",
    "                'ans': ' '.join(dec_version)\n",
    "            })\n",
    "\n",
    "    return pd.DataFrame(rows)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "34043813",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_85/1722440710.py:14: DeprecationWarning: Call to deprecated `__contains__` (Method will be removed in 4.0.0, use self.wv.__contains__() instead).\n",
      "  if pos in pos_tags and tok in wv:\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                que            ans\n",
      "0          12 시 땡 !  하루 가 또 가 네요 .\n",
      "1     1 지망 학교 떨어졌 어    위로 해 드립니다 .\n",
      "2    1 중퇴 학교의 떨어졌 어    위로 해 드립니다 .\n",
      "3    1 중퇴 강습소 떨어졌 어    위로 해 드립니다 .\n",
      "4  1 중퇴 중고등학교 떨어졌 어    위로 해 드립니다 .\n"
     ]
    }
   ],
   "source": [
    "#cross_augmented = augment_pairwise_cross_product(que_corpus, ans_corpus, wv)\n",
    "\n",
    "aug_df = augment_pairwise_cross_product_df(train_que, train_ans, wv_model)\n",
    "print(aug_df.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "6ab9d824",
   "metadata": {},
   "outputs": [],
   "source": [
    "aug_df = aug_df.drop_duplicates('que')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "95ea3a9e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# aug_df = aug_df.drop_duplicates('ans')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "a2a9b004",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>que</th>\n",
       "      <th>ans</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>12 시 땡 !</td>\n",
       "      <td>하루 가 또 가 네요 .</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1 지망 학교 떨어졌 어</td>\n",
       "      <td>위로 해 드립니다 .</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1 중퇴 학교의 떨어졌 어</td>\n",
       "      <td>위로 해 드립니다 .</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1 중퇴 강습소 떨어졌 어</td>\n",
       "      <td>위로 해 드립니다 .</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1 중퇴 중고등학교 떨어졌 어</td>\n",
       "      <td>위로 해 드립니다 .</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>124597</th>\n",
       "      <td>행복 의 편견 을 모르 겠 어</td>\n",
       "      <td>서로 이해 하 고 존중 하 고 아끼 는 마음 이 에요 .</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>124625</th>\n",
       "      <td>행복 의 열등감 을 모르 겠 어</td>\n",
       "      <td>서로 이해 하 고 존중 하 고 아끼 는 마음 이 에요 .</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>124653</th>\n",
       "      <td>절망 의 욕망 을 모르 겠 어</td>\n",
       "      <td>서로 이해 하 고 존중 하 고 아끼 는 마음 이 에요 .</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>124681</th>\n",
       "      <td>절망 의 편견 을 모르 겠 어</td>\n",
       "      <td>서로 이해 하 고 존중 하 고 아끼 는 마음 이 에요 .</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>124709</th>\n",
       "      <td>절망 의 열등감 을 모르 겠 어</td>\n",
       "      <td>서로 이해 하 고 존중 하 고 아끼 는 마음 이 에요 .</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>28083 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                      que                              ans\n",
       "0                12 시 땡 !                    하루 가 또 가 네요 .\n",
       "1           1 지망 학교 떨어졌 어                      위로 해 드립니다 .\n",
       "2          1 중퇴 학교의 떨어졌 어                      위로 해 드립니다 .\n",
       "3          1 중퇴 강습소 떨어졌 어                      위로 해 드립니다 .\n",
       "4        1 중퇴 중고등학교 떨어졌 어                      위로 해 드립니다 .\n",
       "...                   ...                              ...\n",
       "124597   행복 의 편견 을 모르 겠 어  서로 이해 하 고 존중 하 고 아끼 는 마음 이 에요 .\n",
       "124625  행복 의 열등감 을 모르 겠 어  서로 이해 하 고 존중 하 고 아끼 는 마음 이 에요 .\n",
       "124653   절망 의 욕망 을 모르 겠 어  서로 이해 하 고 존중 하 고 아끼 는 마음 이 에요 .\n",
       "124681   절망 의 편견 을 모르 겠 어  서로 이해 하 고 존중 하 고 아끼 는 마음 이 에요 .\n",
       "124709  절망 의 열등감 을 모르 겠 어  서로 이해 하 고 존중 하 고 아끼 는 마음 이 에요 .\n",
       "\n",
       "[28083 rows x 2 columns]"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "aug_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "5f56d56d",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_que = aug_df['que'].to_list()\n",
    "train_ans = aug_df['ans'].to_list()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "127de20b",
   "metadata": {},
   "outputs": [],
   "source": [
    "valid_que = [' '.join([word for word, pos in tokens]) for tokens in valid_que]\n",
    "valid_ans = [' '.join([word for word, pos in tokens]) for tokens in valid_ans]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "89ee39b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def positional_encoding(pos, d_model):\n",
    "    def cal_angle(position, i):\n",
    "        return position / np.power(10000, (2*(i//2)) / np.float32(d_model))\n",
    "\n",
    "    def get_posi_angle_vec(position):\n",
    "        return [cal_angle(position, i) for i in range(d_model)]\n",
    "\n",
    "    sinusoid_table = np.array([get_posi_angle_vec(pos_i) for pos_i in range(pos)])\n",
    "\n",
    "    sinusoid_table[:, 0::2] = np.sin(sinusoid_table[:, 0::2])\n",
    "    sinusoid_table[:, 1::2] = np.cos(sinusoid_table[:, 1::2])\n",
    "\n",
    "    return sinusoid_table\n",
    "\n",
    "def generate_padding_mask(seq):\n",
    "    seq = tf.cast(tf.math.equal(seq, 0), tf.float32)\n",
    "    return seq[:, tf.newaxis, tf.newaxis, :]\n",
    "\n",
    "def generate_lookahead_mask(size):\n",
    "    mask = 1 - tf.linalg.band_part(tf.ones((size, size)), -1, 0)\n",
    "    return mask\n",
    "\n",
    "def generate_masks(src, tgt):\n",
    "    enc_mask = generate_padding_mask(src)\n",
    "    dec_enc_mask = generate_padding_mask(src)\n",
    "\n",
    "    dec_lookahead_mask = generate_lookahead_mask(tgt.shape[1])\n",
    "    dec_tgt_padding_mask = generate_padding_mask(tgt)\n",
    "    dec_mask = tf.maximum(dec_tgt_padding_mask, dec_lookahead_mask)\n",
    "\n",
    "    return enc_mask, dec_enc_mask, dec_mask\n",
    "\n",
    "\n",
    "class MultiHeadAttention(tf.keras.layers.Layer):\n",
    "    def __init__(self, d_model, num_heads):\n",
    "        super(MultiHeadAttention, self).__init__()\n",
    "        self.num_heads = num_heads\n",
    "        self.d_model = d_model\n",
    "        \n",
    "        self.depth = d_model // self.num_heads\n",
    "        \n",
    "        self.W_q = tf.keras.layers.Dense(d_model)\n",
    "        self.W_k = tf.keras.layers.Dense(d_model)\n",
    "        self.W_v = tf.keras.layers.Dense(d_model)\n",
    "        \n",
    "        self.linear = tf.keras.layers.Dense(d_model)\n",
    "\n",
    "    def scaled_dot_product_attention(self, Q, K, V, mask):\n",
    "        d_k = tf.cast(K.shape[-1], tf.float32)\n",
    "        QK = tf.matmul(Q, K, transpose_b=True)\n",
    "\n",
    "        scaled_qk = QK / tf.math.sqrt(d_k)\n",
    "\n",
    "        if mask is not None: scaled_qk += (mask * -1e9)  \n",
    "\n",
    "        attentions = tf.nn.softmax(scaled_qk, axis=-1)\n",
    "        out = tf.matmul(attentions, V)\n",
    "\n",
    "        return out, attentions\n",
    "        \n",
    "\n",
    "    def split_heads(self, x):\n",
    "        bsz = x.shape[0]\n",
    "        split_x = tf.reshape(x, (bsz, -1, self.num_heads, self.depth))\n",
    "        split_x = tf.transpose(split_x, perm=[0, 2, 1, 3])\n",
    "\n",
    "        return split_x\n",
    "\n",
    "    def combine_heads(self, x):\n",
    "        bsz = x.shape[0]\n",
    "        combined_x = tf.transpose(x, perm=[0, 2, 1, 3])\n",
    "        combined_x = tf.reshape(combined_x, (bsz, -1, self.d_model))\n",
    "\n",
    "        return combined_x\n",
    "\n",
    "    \n",
    "    def call(self, Q, K, V, mask):\n",
    "        WQ = self.W_q(Q)\n",
    "        WK = self.W_k(K)\n",
    "        WV = self.W_v(V)\n",
    "        \n",
    "        WQ_splits = self.split_heads(WQ)\n",
    "        WK_splits = self.split_heads(WK)\n",
    "        WV_splits = self.split_heads(WV)\n",
    "        \n",
    "        out, attention_weights = self.scaled_dot_product_attention(\n",
    "            WQ_splits, WK_splits, WV_splits, mask)\n",
    "                        \n",
    "        out = self.combine_heads(out)\n",
    "        out = self.linear(out)\n",
    "            \n",
    "        return out, attention_weights\n",
    "\n",
    "    \n",
    "class PoswiseFeedForwardNet(tf.keras.layers.Layer):\n",
    "    def __init__(self, d_model, d_ff):\n",
    "        super(PoswiseFeedForwardNet, self).__init__()\n",
    "        self.d_model = d_model\n",
    "        self.d_ff = d_ff\n",
    "\n",
    "        self.fc1 = tf.keras.layers.Dense(d_ff, activation='relu')\n",
    "        self.fc2 = tf.keras.layers.Dense(d_model)\n",
    "\n",
    "    def call(self, x):\n",
    "        out = self.fc1(x)\n",
    "        out = self.fc2(out)\n",
    "            \n",
    "        return out\n",
    "    \n",
    "    \n",
    "class EncoderLayer(tf.keras.layers.Layer):\n",
    "    def __init__(self, d_model, n_heads, d_ff, dropout):\n",
    "        super(EncoderLayer, self).__init__()\n",
    "\n",
    "        self.enc_self_attn = MultiHeadAttention(d_model, n_heads)\n",
    "        self.ffn = PoswiseFeedForwardNet(d_model, d_ff)\n",
    "\n",
    "        self.norm_1 = tf.keras.layers.LayerNormalization(epsilon=1e-6)\n",
    "        self.norm_2 = tf.keras.layers.LayerNormalization(epsilon=1e-6)\n",
    "\n",
    "        self.do = tf.keras.layers.Dropout(dropout)\n",
    "        \n",
    "    def call(self, x, mask):\n",
    "        '''\n",
    "        Multi-Head Attention\n",
    "        '''\n",
    "        residual = x\n",
    "        out = self.norm_1(x)\n",
    "        out, enc_attn = self.enc_self_attn(out, out, out, mask)\n",
    "        out = self.do(out)\n",
    "        out += residual\n",
    "        \n",
    "        '''\n",
    "        Position-Wise Feed Forward Network\n",
    "        '''\n",
    "        residual = out\n",
    "        out = self.norm_2(out)\n",
    "        out = self.ffn(out)\n",
    "        out = self.do(out)\n",
    "        out += residual\n",
    "        \n",
    "        return out, enc_attn\n",
    "    \n",
    "    \n",
    "class DecoderLayer(tf.keras.layers.Layer):\n",
    "    def __init__(self, d_model, num_heads, d_ff, dropout):\n",
    "        super(DecoderLayer, self).__init__()\n",
    "\n",
    "        self.dec_self_attn = MultiHeadAttention(d_model, num_heads)\n",
    "        self.enc_dec_attn = MultiHeadAttention(d_model, num_heads)\n",
    "\n",
    "        self.ffn = PoswiseFeedForwardNet(d_model, d_ff)\n",
    "\n",
    "        self.norm_1 = tf.keras.layers.LayerNormalization(epsilon=1e-6)\n",
    "        self.norm_2 = tf.keras.layers.LayerNormalization(epsilon=1e-6)\n",
    "        self.norm_3 = tf.keras.layers.LayerNormalization(epsilon=1e-6)\n",
    "\n",
    "        self.do = tf.keras.layers.Dropout(dropout)\n",
    "    \n",
    "    def call(self, x, enc_out, dec_enc_mask, padding_mask):\n",
    "        '''\n",
    "        Masked Multi-Head Attention\n",
    "        '''\n",
    "        residual = x\n",
    "        out = self.norm_1(x)\n",
    "        out, dec_attn = self.dec_self_attn(out, out, out, padding_mask)\n",
    "        out = self.do(out)\n",
    "        out += residual\n",
    "\n",
    "        '''\n",
    "        Multi-Head Attention\n",
    "        '''\n",
    "        residual = out\n",
    "        out = self.norm_2(out)\n",
    "        # Q, K, V 순서에 주의하세요!\n",
    "        out, dec_enc_attn = self.enc_dec_attn(Q=out, K=enc_out, V=enc_out, mask=dec_enc_mask)\n",
    "        out = self.do(out)\n",
    "        out += residual\n",
    "        \n",
    "        '''\n",
    "        Position-Wise Feed Forward Network\n",
    "        '''\n",
    "        residual = out\n",
    "        out = self.norm_3(out)\n",
    "        out = self.ffn(out)\n",
    "        out = self.do(out)\n",
    "        out += residual\n",
    "\n",
    "        return out, dec_attn, dec_enc_attn\n",
    "    \n",
    "\n",
    "class Encoder(tf.keras.Model):\n",
    "    def __init__(self,\n",
    "                    n_layers,\n",
    "                    d_model,\n",
    "                    n_heads,\n",
    "                    d_ff,\n",
    "                    dropout):\n",
    "        super(Encoder, self).__init__()\n",
    "        self.n_layers = n_layers\n",
    "        self.enc_layers = [EncoderLayer(d_model, n_heads, d_ff, dropout) \n",
    "                        for _ in range(n_layers)]\n",
    "    \n",
    "        self.do = tf.keras.layers.Dropout(dropout)\n",
    "        \n",
    "    def call(self, x, mask):\n",
    "        out = x\n",
    "    \n",
    "        enc_attns = list()\n",
    "        for i in range(self.n_layers):\n",
    "            out, enc_attn = self.enc_layers[i](out, mask)\n",
    "            enc_attns.append(enc_attn)\n",
    "        \n",
    "        return out, enc_attns\n",
    "    \n",
    "    \n",
    "class Decoder(tf.keras.Model):\n",
    "    def __init__(self,\n",
    "                    n_layers,\n",
    "                    d_model,\n",
    "                    n_heads,\n",
    "                    d_ff,\n",
    "                    dropout):\n",
    "        super(Decoder, self).__init__()\n",
    "        self.n_layers = n_layers\n",
    "        self.dec_layers = [DecoderLayer(d_model, n_heads, d_ff, dropout) \n",
    "                            for _ in range(n_layers)]\n",
    "                            \n",
    "    def call(self, x, enc_out, dec_enc_mask, padding_mask):\n",
    "        out = x\n",
    "    \n",
    "        dec_attns = list()\n",
    "        dec_enc_attns = list()\n",
    "        for i in range(self.n_layers):\n",
    "            out, dec_attn, dec_enc_attn = \\\n",
    "            self.dec_layers[i](out, enc_out, dec_enc_mask, padding_mask)\n",
    "\n",
    "            dec_attns.append(dec_attn)\n",
    "            dec_enc_attns.append(dec_enc_attn)\n",
    "\n",
    "        return out, dec_attns, dec_enc_attns\n",
    "    \n",
    "    \n",
    "class Transformer(tf.keras.Model):\n",
    "    def __init__(self,\n",
    "                    n_layers,\n",
    "                    d_model,\n",
    "                    n_heads,\n",
    "                    d_ff,\n",
    "                    src_vocab_size,\n",
    "                    tgt_vocab_size,\n",
    "                    pos_len,\n",
    "                    dropout=0.2,\n",
    "                    shared_fc=True,\n",
    "                    shared_emb=False):\n",
    "        super(Transformer, self).__init__()\n",
    "        \n",
    "        self.d_model = tf.cast(d_model, tf.float32)\n",
    "\n",
    "        if shared_emb:\n",
    "            self.enc_emb = self.dec_emb = \\\n",
    "            tf.keras.layers.Embedding(src_vocab_size, d_model)\n",
    "        else:\n",
    "            self.enc_emb = tf.keras.layers.Embedding(src_vocab_size, d_model)\n",
    "            self.dec_emb = tf.keras.layers.Embedding(tgt_vocab_size, d_model)\n",
    "\n",
    "        self.pos_encoding = positional_encoding(pos_len, d_model)\n",
    "        self.do = tf.keras.layers.Dropout(dropout)\n",
    "\n",
    "        self.encoder = Encoder(n_layers, d_model, n_heads, d_ff, dropout)\n",
    "        self.decoder = Decoder(n_layers, d_model, n_heads, d_ff, dropout)\n",
    "\n",
    "        self.fc = tf.keras.layers.Dense(tgt_vocab_size)\n",
    "\n",
    "        self.shared_fc = shared_fc\n",
    "\n",
    "        if shared_fc:\n",
    "            self.fc.set_weights(tf.transpose(self.dec_emb.weights))\n",
    "\n",
    "    def embedding(self, emb, x):\n",
    "        seq_len = x.shape[1]\n",
    "\n",
    "        out = emb(x)\n",
    "\n",
    "        if self.shared_fc: out *= tf.math.sqrt(self.d_model)\n",
    "\n",
    "        out += self.pos_encoding[np.newaxis, ...][:, :seq_len, :]\n",
    "        out = self.do(out)\n",
    "\n",
    "        return out\n",
    "\n",
    "        \n",
    "    def call(self, enc_in, dec_in, enc_mask, dec_enc_mask, dec_mask):\n",
    "        enc_in = self.embedding(self.enc_emb, enc_in)\n",
    "        dec_in = self.embedding(self.dec_emb, dec_in)\n",
    "\n",
    "        enc_out, enc_attns = self.encoder(enc_in, enc_mask)\n",
    "        \n",
    "        dec_out, dec_attns, dec_enc_attns = \\\n",
    "        self.decoder(dec_in, enc_out, dec_enc_mask, dec_mask)\n",
    "        \n",
    "        logits = self.fc(dec_out)\n",
    "        \n",
    "        return logits, enc_attns, dec_attns, dec_enc_attns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "4f34a4f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "transformer = Transformer(\n",
    "    n_layers=2,\n",
    "    d_model=512,\n",
    "    n_heads=8,\n",
    "    d_ff=2048,\n",
    "    src_vocab_size=VOCAB_SIZE,\n",
    "    tgt_vocab_size=VOCAB_SIZE,\n",
    "    pos_len=200,\n",
    "    dropout=0.3,\n",
    "    shared_fc=True,\n",
    "    shared_emb=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "02c4e02f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(28083, 28083, 1547, 1547)"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(train_que), len(train_ans), len(valid_que), len(valid_ans)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "8f061863",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_ans = [f\"<start> {ans} <end>\" for ans in train_ans]\n",
    "valid_ans = [f\"<start> {ans} <end>\" for ans in valid_ans]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "5bd8a09b",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_que = [sentence.split() for sentence in train_que]\n",
    "train_ans = [sentence.split() for sentence in train_ans]\n",
    "valid_que = [sentence.split() for sentence in valid_que]\n",
    "valid_ans = [sentence.split() for sentence in valid_ans]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "29316834",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "tokenizer = Tokenizer(oov_token=\"<OOV>\")\n",
    "all_sentences = train_que + train_ans + valid_que + valid_ans  # 학습할 문장 합치기\n",
    "tokenizer.fit_on_texts(all_sentences)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "185afc3e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "단어 인덱스: 7699\n"
     ]
    }
   ],
   "source": [
    "word_index = tokenizer.word_index\n",
    "print(\"단어 인덱스:\", len(word_index))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "6a5f1a08",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_que_tokens = tokenizer.texts_to_sequences(train_que)\n",
    "train_ans_tokens = tokenizer.texts_to_sequences(train_ans)\n",
    "valid_que_tokens = tokenizer.texts_to_sequences(valid_que)\n",
    "valid_ans_tokens = tokenizer.texts_to_sequences(valid_ans)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "56ed5877",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.preprocessing.sequence import pad_sequences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "id": "056db0e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "BATCH_SIZE = 64\n",
    "train_que_padded = pad_sequences(train_que_tokens, padding='post')\n",
    "train_ans_padded = pad_sequences(train_ans_tokens, padding='post')\n",
    "train_dataset = tf.data.Dataset.from_tensor_slices((train_que_padded, train_ans_padded)).batch(batch_size=BATCH_SIZE)\n",
    "\n",
    "valid_que_padded = pad_sequences(valid_que_tokens, padding='post')\n",
    "valid_ans_padded = pad_sequences(valid_ans_tokens, padding='post')\n",
    "# tf.data.Dataset을 사용하여 배치 처리\n",
    "valid_dataset = tf.data.Dataset.from_tensor_slices((valid_que_padded, valid_ans_padded)).batch(batch_size=BATCH_SIZE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "id": "cff4b857",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<BatchDataset shapes: ((None, 28), (None, 37)), types: (tf.int32, tf.int32)>"
      ]
     },
     "execution_count": 79,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "id": "6762750e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<BatchDataset shapes: ((None, 32), (None, 42)), types: (tf.int32, tf.int32)>"
      ]
     },
     "execution_count": 80,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "valid_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "id": "84ed024c",
   "metadata": {},
   "outputs": [],
   "source": [
    "loss_object = tf.keras.losses.SparseCategoricalCrossentropy(\n",
    "    from_logits=True, reduction='none')\n",
    "\n",
    "def loss_function(real, pred):\n",
    "    mask = tf.math.logical_not(tf.math.equal(real, 0))\n",
    "    loss_ = loss_object(real, pred)\n",
    "\n",
    "    mask = tf.cast(mask, dtype=loss_.dtype)\n",
    "    loss_ *= mask\n",
    "\n",
    "    return tf.reduce_sum(loss_)/tf.reduce_sum(mask)\n",
    "\n",
    "@tf.function\n",
    "def train_step(src, tgt, model, optimizer):\n",
    "    tgt_in = tgt[:, :-1]  # Decoder의 input\n",
    "    gold = tgt[:, 1:]     # Decoder의 output과 비교하기 위해 right shift를 통해 생성한 최종 타겟\n",
    "\n",
    "    enc_mask, dec_enc_mask, dec_mask = generate_masks(src, tgt_in)\n",
    "\n",
    "    with tf.GradientTape() as tape:\n",
    "        predictions, enc_attns, dec_attns, dec_enc_attns = \\\n",
    "            model(src, tgt_in, enc_mask, dec_enc_mask, dec_mask)\n",
    "        loss = loss_function(gold, predictions)\n",
    "\n",
    "    gradients = tape.gradient(loss, model.trainable_variables)\n",
    "    optimizer.apply_gradients(zip(gradients, model.trainable_variables))\n",
    "\n",
    "    return loss, enc_attns, dec_attns, dec_enc_attns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "id": "e61a12a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(model, train_dataset, valid_dataset, optimizer, num_epochs=10, patience=3):\n",
    "    best_valid_loss = float('inf')  # 최저 검증 손실을 저장\n",
    "    epochs_without_improvement = 0  # 연속적으로 검증 손실이 개선되지 않은 에폭 수\n",
    "    for epoch in range(num_epochs):\n",
    "        print(f\"Epoch {epoch + 1}/{num_epochs}\")\n",
    "        total_loss = 0\n",
    "        for step, (src, tgt) in enumerate(train_dataset):\n",
    "            # 훈련 step 수행\n",
    "            loss, enc_attns, dec_attns, dec_enc_attns = train_step(src, tgt, model, optimizer)\n",
    "            total_loss += loss\n",
    "\n",
    "        avg_train_loss = total_loss / (step + 1)  # 평균 훈련 손실\n",
    "        print(f\"Train Loss: {avg_train_loss.numpy()}\")\n",
    "\n",
    "        # Validation loss 계산\n",
    "        valid_loss = validate(model, valid_dataset)\n",
    "        print(f\"Validation Loss: {valid_loss.numpy()}\")\n",
    "\n",
    "        # Early stopping check\n",
    "        if valid_loss < best_valid_loss:\n",
    "            best_valid_loss = valid_loss\n",
    "            epochs_without_improvement = 0\n",
    "        else:\n",
    "            epochs_without_improvement += 1\n",
    "\n",
    "        if epochs_without_improvement >= patience:\n",
    "            print(f\"Early stopping triggered after {epoch + 1} epochs.\")\n",
    "            break\n",
    "\n",
    "def validate(model, valid_dataset):\n",
    "    total_valid_loss = 0\n",
    "    for src, tgt in valid_dataset:\n",
    "        tgt_in = tgt[:, :-1]  # Decoder의 input\n",
    "        gold = tgt[:, 1:]     # Decoder의 output과 비교하기 위해 right shift를 통해 생성한 최종 타겟\n",
    "        enc_mask, dec_enc_mask, dec_mask = generate_masks(src, tgt_in)\n",
    "\n",
    "        predictions, enc_attns, dec_attns, dec_enc_attns = model(src, tgt_in, enc_mask, dec_enc_mask, dec_mask)\n",
    "        valid_loss = loss_function(gold, predictions)\n",
    "        total_valid_loss += valid_loss\n",
    "\n",
    "    avg_valid_loss = total_valid_loss / len(valid_dataset)  # 평균 검증 손실\n",
    "    return avg_valid_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "id": "595cae4f",
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = tf.keras.optimizers.Adam(learning_rate=1e-4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "id": "fb017ec8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "Train Loss: 4.631084442138672\n",
      "Validation Loss: 4.357891082763672\n",
      "Epoch 2/10\n",
      "Train Loss: 3.2934648990631104\n",
      "Validation Loss: 4.25074577331543\n",
      "Epoch 3/10\n",
      "Train Loss: 2.5585858821868896\n",
      "Validation Loss: 4.278972625732422\n",
      "Epoch 4/10\n",
      "Train Loss: 1.9226468801498413\n",
      "Validation Loss: 4.316685676574707\n",
      "Epoch 5/10\n",
      "Train Loss: 1.4241939783096313\n",
      "Validation Loss: 4.511600971221924\n",
      "Early stopping triggered after 5 epochs.\n"
     ]
    }
   ],
   "source": [
    "train(transformer, train_dataset, valid_dataset, optimizer, num_epochs=10, patience=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "61864449",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c4acbd3",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c8f882d9",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "84f66c11",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de9d7398",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
