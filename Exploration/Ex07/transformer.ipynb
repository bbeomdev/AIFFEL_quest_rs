{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 206,
   "id": "f07df1a1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "슝=3\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "import tensorflow_datasets as tfds\n",
    "import os\n",
    "import re\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 207,
   "id": "d0d4f1b7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "슝=3\n"
     ]
    }
   ],
   "source": [
    "# 포지셔널 인코딩 레이어\n",
    "\n",
    "class PositionalEncoding(tf.keras.layers.Layer):\n",
    "\n",
    "  def __init__(self, position, d_model):\n",
    "    super(PositionalEncoding, self).__init__()\n",
    "    self.pos_encoding = self.positional_encoding(position, d_model)\n",
    "\n",
    "  def get_angles(self, position, i, d_model):\n",
    "    angles = 1 / tf.pow(10000, (2 * (i // 2)) / tf.cast(d_model, tf.float32))\n",
    "    return position * angles\n",
    "\n",
    "  def positional_encoding(self, position, d_model):\n",
    "    # 각도 배열 생성\n",
    "    angle_rads = self.get_angles(\n",
    "        position=tf.range(position, dtype=tf.float32)[:, tf.newaxis],\n",
    "        i=tf.range(d_model, dtype=tf.float32)[tf.newaxis, :],\n",
    "        d_model=d_model)\n",
    "\n",
    "    # 배열의 짝수 인덱스에는 sin 함수 적용\n",
    "    sines = tf.math.sin(angle_rads[:, 0::2])\n",
    "    # 배열의 홀수 인덱스에는 cosine 함수 적용\n",
    "    cosines = tf.math.cos(angle_rads[:, 1::2])\n",
    "\n",
    "    # sin과 cosine이 교차되도록 재배열\n",
    "    pos_encoding = tf.stack([sines, cosines], axis=0)\n",
    "    pos_encoding = tf.transpose(pos_encoding,[1, 2, 0]) \n",
    "    pos_encoding = tf.reshape(pos_encoding, [position, d_model])\n",
    "\n",
    "    pos_encoding = pos_encoding[tf.newaxis, ...]\n",
    "    return tf.cast(pos_encoding, tf.float32)\n",
    "\n",
    "  def call(self, inputs):\n",
    "    return inputs + self.pos_encoding[:, :tf.shape(inputs)[1], :]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 208,
   "id": "c79c0d78",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "슝=3\n"
     ]
    }
   ],
   "source": [
    "# 스케일드 닷 프로덕트 어텐션 함수\n",
    "\n",
    "def scaled_dot_product_attention(query, key, value, mask):\n",
    "  # 어텐션 가중치는 Q와 K의 닷 프로덕트\n",
    "  matmul_qk = tf.matmul(query, key, transpose_b=True)\n",
    "\n",
    "  # 가중치를 정규화\n",
    "  depth = tf.cast(tf.shape(key)[-1], tf.float32)\n",
    "  logits = matmul_qk / tf.math.sqrt(depth)\n",
    "\n",
    "  # 패딩에 마스크 추가\n",
    "  if mask is not None:\n",
    "    logits += (mask * -1e9)\n",
    "\n",
    "  # softmax적용\n",
    "  attention_weights = tf.nn.softmax(logits, axis=-1)\n",
    "\n",
    "  # 최종 어텐션은 가중치와 V의 닷 프로덕트\n",
    "  output = tf.matmul(attention_weights, value)\n",
    "  return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 209,
   "id": "b6d50de0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "슝=3\n"
     ]
    }
   ],
   "source": [
    "class MultiHeadAttention(tf.keras.layers.Layer):\n",
    "  def __init__(self, d_model, num_heads, name=\"multi_head_attention\"):\n",
    "    super(MultiHeadAttention, self).__init__(name=name)\n",
    "    self.num_heads = num_heads\n",
    "    self.d_model = d_model\n",
    "\n",
    "    assert d_model % self.num_heads == 0\n",
    "\n",
    "    self.depth = d_model // self.num_heads\n",
    "\n",
    "    self.query_dense = tf.keras.layers.Dense(units=d_model)\n",
    "    self.key_dense = tf.keras.layers.Dense(units=d_model)\n",
    "    self.value_dense = tf.keras.layers.Dense(units=d_model)\n",
    "\n",
    "    self.dense = tf.keras.layers.Dense(units=d_model)\n",
    "\n",
    "  def split_heads(self, inputs, batch_size):\n",
    "    inputs = tf.reshape(\n",
    "        inputs, shape=(batch_size, -1, self.num_heads, self.depth))\n",
    "    return tf.transpose(inputs, perm=[0, 2, 1, 3])\n",
    "\n",
    "  def call(self, inputs):\n",
    "    query, key, value, mask = inputs['query'], inputs['key'], inputs[\n",
    "        'value'], inputs['mask']\n",
    "    batch_size = tf.shape(query)[0]\n",
    "\n",
    "    # Q, K, V에 각각 Dense를 적용합니다\n",
    "    query = self.query_dense(query)\n",
    "    key = self.key_dense(key)\n",
    "    value = self.value_dense(value)\n",
    "\n",
    "    # 병렬 연산을 위한 머리를 여러 개 만듭니다\n",
    "    query = self.split_heads(query, batch_size)\n",
    "    key = self.split_heads(key, batch_size)\n",
    "    value = self.split_heads(value, batch_size)\n",
    "\n",
    "    # 스케일드 닷 프로덕트 어텐션 함수 사용\n",
    "    scaled_attention = scaled_dot_product_attention(query, key, value, mask)\n",
    "\n",
    "    scaled_attention = tf.transpose(scaled_attention, perm=[0, 2, 1, 3])\n",
    "\n",
    "    # 어텐션 연산 후에 각 결과를 다시 연결(concatenate)합니다\n",
    "    concat_attention = tf.reshape(scaled_attention,\n",
    "                                  (batch_size, -1, self.d_model))\n",
    "\n",
    "    # 최종 결과에도 Dense를 한 번 더 적용합니다\n",
    "    outputs = self.dense(concat_attention)\n",
    "\n",
    "    return outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 210,
   "id": "73d1d276",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "슝=3\n"
     ]
    }
   ],
   "source": [
    "def create_padding_mask(x):\n",
    "  mask = tf.cast(tf.math.equal(x, 0), tf.float32)\n",
    "  # (batch_size, 1, 1, sequence length)\n",
    "  return mask[:, tf.newaxis, tf.newaxis, :]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 211,
   "id": "4c46f731",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "슝=3\n"
     ]
    }
   ],
   "source": [
    "def create_look_ahead_mask(x):\n",
    "  seq_len = tf.shape(x)[1]\n",
    "  look_ahead_mask = 1 - tf.linalg.band_part(tf.ones((seq_len, seq_len)), -1, 0)\n",
    "  padding_mask = create_padding_mask(x)\n",
    "  return tf.maximum(look_ahead_mask, padding_mask)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 212,
   "id": "c36a804d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "슝=3\n"
     ]
    }
   ],
   "source": [
    "# 인코더 하나의 레이어를 함수로 구현.\n",
    "# 이 하나의 레이어 안에는 두 개의 서브 레이어가 존재합니다.\n",
    "def encoder_layer(units, d_model, num_heads, dropout, name=\"encoder_layer\"):\n",
    "  inputs = tf.keras.Input(shape=(None, d_model), name=\"inputs\")\n",
    "\n",
    "  # 패딩 마스크 사용\n",
    "  padding_mask = tf.keras.Input(shape=(1, 1, None), name=\"padding_mask\")\n",
    "\n",
    "  # 첫 번째 서브 레이어 : 멀티 헤드 어텐션 수행 (셀프 어텐션)\n",
    "  attention = MultiHeadAttention(\n",
    "      d_model, num_heads, name=\"attention\")({\n",
    "          'query': inputs,\n",
    "          'key': inputs,\n",
    "          'value': inputs,\n",
    "          'mask': padding_mask\n",
    "      })\n",
    "\n",
    "  # 어텐션의 결과는 Dropout과 Layer Normalization이라는 훈련을 돕는 테크닉을 수행\n",
    "  attention = tf.keras.layers.Dropout(rate=dropout)(attention)\n",
    "  attention = tf.keras.layers.LayerNormalization(\n",
    "      epsilon=1e-6)(inputs + attention)\n",
    "\n",
    "  # 두 번째 서브 레이어 : 2개의 완전연결층\n",
    "  outputs = tf.keras.layers.Dense(units=units, activation='relu')(attention)\n",
    "  outputs = tf.keras.layers.Dense(units=d_model)(outputs)\n",
    "\n",
    "  # 완전연결층의 결과는 Dropout과 LayerNormalization이라는 훈련을 돕는 테크닉을 수행\n",
    "  outputs = tf.keras.layers.Dropout(rate=dropout)(outputs)\n",
    "  outputs = tf.keras.layers.LayerNormalization(\n",
    "      epsilon=1e-6)(attention + outputs)\n",
    "\n",
    "  return tf.keras.Model(\n",
    "      inputs=[inputs, padding_mask], outputs=outputs, name=name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 213,
   "id": "a15b3424",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "슝=3\n"
     ]
    }
   ],
   "source": [
    "def encoder(vocab_size,\n",
    "            num_layers,\n",
    "            units,\n",
    "            d_model,\n",
    "            num_heads,\n",
    "            dropout,\n",
    "            name=\"encoder\"):\n",
    "  inputs = tf.keras.Input(shape=(None,), name=\"inputs\")\n",
    "\n",
    "  # 패딩 마스크 사용\n",
    "  padding_mask = tf.keras.Input(shape=(1, 1, None), name=\"padding_mask\")\n",
    "\n",
    "  # 임베딩 레이어\n",
    "  embeddings = tf.keras.layers.Embedding(vocab_size, d_model)(inputs)\n",
    "  embeddings *= tf.math.sqrt(tf.cast(d_model, tf.float32))\n",
    "\n",
    "  # 포지셔널 인코딩\n",
    "  embeddings = PositionalEncoding(vocab_size, d_model)(embeddings)\n",
    "\n",
    "  outputs = tf.keras.layers.Dropout(rate=dropout)(embeddings)\n",
    "\n",
    "  # num_layers만큼 쌓아올린 인코더의 층.\n",
    "  for i in range(num_layers):\n",
    "    outputs = encoder_layer(\n",
    "        units=units,\n",
    "        d_model=d_model,\n",
    "        num_heads=num_heads,\n",
    "        dropout=dropout,\n",
    "        name=\"encoder_layer_{}\".format(i),\n",
    "    )([outputs, padding_mask])\n",
    "\n",
    "  return tf.keras.Model(\n",
    "      inputs=[inputs, padding_mask], outputs=outputs, name=name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "id": "b8c91226",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "슝=3\n"
     ]
    }
   ],
   "source": [
    "# 디코더 하나의 레이어를 함수로 구현.\n",
    "# 이 하나의 레이어 안에는 세 개의 서브 레이어가 존재합니다.\n",
    "def decoder_layer(units, d_model, num_heads, dropout, name=\"decoder_layer\"):\n",
    "  inputs = tf.keras.Input(shape=(None, d_model), name=\"inputs\")\n",
    "  enc_outputs = tf.keras.Input(shape=(None, d_model), name=\"encoder_outputs\")\n",
    "  look_ahead_mask = tf.keras.Input(\n",
    "      shape=(1, None, None), name=\"look_ahead_mask\")\n",
    "  padding_mask = tf.keras.Input(shape=(1, 1, None), name='padding_mask')\n",
    "\n",
    "  # 첫 번째 서브 레이어 : 멀티 헤드 어텐션 수행 (셀프 어텐션)\n",
    "  attention1 = MultiHeadAttention(\n",
    "      d_model, num_heads, name=\"attention_1\")(inputs={\n",
    "          'query': inputs,\n",
    "          'key': inputs,\n",
    "          'value': inputs,\n",
    "          'mask': look_ahead_mask\n",
    "      })\n",
    "\n",
    "  # 멀티 헤드 어텐션의 결과는 LayerNormalization이라는 훈련을 돕는 테크닉을 수행\n",
    "  attention1 = tf.keras.layers.LayerNormalization(\n",
    "      epsilon=1e-6)(attention1 + inputs)\n",
    "\n",
    "  # 두 번째 서브 레이어 : 마스크드 멀티 헤드 어텐션 수행 (인코더-디코더 어텐션)\n",
    "  attention2 = MultiHeadAttention(\n",
    "      d_model, num_heads, name=\"attention_2\")(inputs={\n",
    "          'query': attention1,\n",
    "          'key': enc_outputs,\n",
    "          'value': enc_outputs,\n",
    "          'mask': padding_mask\n",
    "      })\n",
    "\n",
    "  # 마스크드 멀티 헤드 어텐션의 결과는\n",
    "  # Dropout과 LayerNormalization이라는 훈련을 돕는 테크닉을 수행\n",
    "  attention2 = tf.keras.layers.Dropout(rate=dropout)(attention2)\n",
    "  attention2 = tf.keras.layers.LayerNormalization(\n",
    "      epsilon=1e-6)(attention2 + attention1)\n",
    "\n",
    "  # 세 번째 서브 레이어 : 2개의 완전연결층\n",
    "  outputs = tf.keras.layers.Dense(units=units, activation='relu')(attention2)\n",
    "  outputs = tf.keras.layers.Dense(units=d_model)(outputs)\n",
    "\n",
    "  # 완전연결층의 결과는 Dropout과 LayerNormalization 수행\n",
    "  outputs = tf.keras.layers.Dropout(rate=dropout)(outputs)\n",
    "  outputs = tf.keras.layers.LayerNormalization(\n",
    "      epsilon=1e-6)(outputs + attention2)\n",
    "\n",
    "  return tf.keras.Model(\n",
    "      inputs=[inputs, enc_outputs, look_ahead_mask, padding_mask],\n",
    "      outputs=outputs,\n",
    "      name=name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "id": "b6a0c7cd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "슝=3\n"
     ]
    }
   ],
   "source": [
    "def decoder(vocab_size,\n",
    "            num_layers,\n",
    "            units,\n",
    "            d_model,\n",
    "            num_heads,\n",
    "            dropout,\n",
    "            name='decoder'):\n",
    "  inputs = tf.keras.Input(shape=(None,), name='inputs')\n",
    "  enc_outputs = tf.keras.Input(shape=(None, d_model), name='encoder_outputs')\n",
    "  look_ahead_mask = tf.keras.Input(\n",
    "      shape=(1, None, None), name='look_ahead_mask')\n",
    "\n",
    "  # 패딩 마스크\n",
    "  padding_mask = tf.keras.Input(shape=(1, 1, None), name='padding_mask')\n",
    "  \n",
    "  # 임베딩 레이어\n",
    "  embeddings = tf.keras.layers.Embedding(vocab_size, d_model)(inputs)\n",
    "  embeddings *= tf.math.sqrt(tf.cast(d_model, tf.float32))\n",
    "\n",
    "  # 포지셔널 인코딩\n",
    "  embeddings = PositionalEncoding(vocab_size, d_model)(embeddings)\n",
    "\n",
    "  # Dropout이라는 훈련을 돕는 테크닉을 수행\n",
    "  outputs = tf.keras.layers.Dropout(rate=dropout)(embeddings)\n",
    "\n",
    "  for i in range(num_layers):\n",
    "    outputs = decoder_layer(\n",
    "        units=units,\n",
    "        d_model=d_model,\n",
    "        num_heads=num_heads,\n",
    "        dropout=dropout,\n",
    "        name='decoder_layer_{}'.format(i),\n",
    "    )(inputs=[outputs, enc_outputs, look_ahead_mask, padding_mask])\n",
    "\n",
    "  return tf.keras.Model(\n",
    "      inputs=[inputs, enc_outputs, look_ahead_mask, padding_mask],\n",
    "      outputs=outputs,\n",
    "      name=name)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "30d30b64",
   "metadata": {},
   "source": [
    "데이터의 출처입니다.\n",
    "\n",
    "https://github.com/songys/Chatbot_data/blob/master/ChatbotData.csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 219,
   "id": "5a903178",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_row = pd.read_csv('ChatbotData.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "id": "7f382ff7",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = data_row.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "id": "a6a4782b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 11823 entries, 0 to 11822\n",
      "Data columns (total 3 columns):\n",
      " #   Column  Non-Null Count  Dtype \n",
      "---  ------  --------------  ----- \n",
      " 0   Q       11823 non-null  object\n",
      " 1   A       11823 non-null  object\n",
      " 2   label   11823 non-null  int64 \n",
      "dtypes: int64(1), object(2)\n",
      "memory usage: 277.2+ KB\n"
     ]
    }
   ],
   "source": [
    "data.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "id": "205803b4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Q</th>\n",
       "      <th>A</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>12시 땡!</td>\n",
       "      <td>하루가 또 가네요.</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1지망 학교 떨어졌어</td>\n",
       "      <td>위로해 드립니다.</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3박4일 놀러가고 싶다</td>\n",
       "      <td>여행은 언제나 좋죠.</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3박4일 정도 놀러가고 싶다</td>\n",
       "      <td>여행은 언제나 좋죠.</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>PPL 심하네</td>\n",
       "      <td>눈살이 찌푸려지죠.</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11818</th>\n",
       "      <td>훔쳐보는 것도 눈치 보임.</td>\n",
       "      <td>티가 나니까 눈치가 보이는 거죠!</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11819</th>\n",
       "      <td>훔쳐보는 것도 눈치 보임.</td>\n",
       "      <td>훔쳐보는 거 티나나봐요.</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11820</th>\n",
       "      <td>흑기사 해주는 짝남.</td>\n",
       "      <td>설렜겠어요.</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11821</th>\n",
       "      <td>힘든 연애 좋은 연애라는게 무슨 차이일까?</td>\n",
       "      <td>잘 헤어질 수 있는 사이 여부인 거 같아요.</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11822</th>\n",
       "      <td>힘들어서 결혼할까봐</td>\n",
       "      <td>도피성 결혼은 하지 않길 바라요.</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>11823 rows × 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                             Q                         A  label\n",
       "0                       12시 땡!                하루가 또 가네요.      0\n",
       "1                  1지망 학교 떨어졌어                 위로해 드립니다.      0\n",
       "2                 3박4일 놀러가고 싶다               여행은 언제나 좋죠.      0\n",
       "3              3박4일 정도 놀러가고 싶다               여행은 언제나 좋죠.      0\n",
       "4                      PPL 심하네                눈살이 찌푸려지죠.      0\n",
       "...                        ...                       ...    ...\n",
       "11818           훔쳐보는 것도 눈치 보임.        티가 나니까 눈치가 보이는 거죠!      2\n",
       "11819           훔쳐보는 것도 눈치 보임.             훔쳐보는 거 티나나봐요.      2\n",
       "11820              흑기사 해주는 짝남.                    설렜겠어요.      2\n",
       "11821  힘든 연애 좋은 연애라는게 무슨 차이일까?  잘 헤어질 수 있는 사이 여부인 거 같아요.      2\n",
       "11822               힘들어서 결혼할까봐        도피성 결혼은 하지 않길 바라요.      2\n",
       "\n",
       "[11823 rows x 3 columns]"
      ]
     },
     "execution_count": 89,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "id": "16e42cac",
   "metadata": {},
   "outputs": [],
   "source": [
    "data.dropna(inplace=True) # 결측치 제거"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "id": "435e8c8e",
   "metadata": {},
   "outputs": [],
   "source": [
    "data.drop_duplicates(inplace=True) # 중복 제거"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "id": "9b2dbcd8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Q</th>\n",
       "      <th>A</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>12시 땡!</td>\n",
       "      <td>하루가 또 가네요.</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1지망 학교 떨어졌어</td>\n",
       "      <td>위로해 드립니다.</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3박4일 놀러가고 싶다</td>\n",
       "      <td>여행은 언제나 좋죠.</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3박4일 정도 놀러가고 싶다</td>\n",
       "      <td>여행은 언제나 좋죠.</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>PPL 심하네</td>\n",
       "      <td>눈살이 찌푸려지죠.</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11818</th>\n",
       "      <td>훔쳐보는 것도 눈치 보임.</td>\n",
       "      <td>티가 나니까 눈치가 보이는 거죠!</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11819</th>\n",
       "      <td>훔쳐보는 것도 눈치 보임.</td>\n",
       "      <td>훔쳐보는 거 티나나봐요.</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11820</th>\n",
       "      <td>흑기사 해주는 짝남.</td>\n",
       "      <td>설렜겠어요.</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11821</th>\n",
       "      <td>힘든 연애 좋은 연애라는게 무슨 차이일까?</td>\n",
       "      <td>잘 헤어질 수 있는 사이 여부인 거 같아요.</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11822</th>\n",
       "      <td>힘들어서 결혼할까봐</td>\n",
       "      <td>도피성 결혼은 하지 않길 바라요.</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>11823 rows × 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                             Q                         A  label\n",
       "0                       12시 땡!                하루가 또 가네요.      0\n",
       "1                  1지망 학교 떨어졌어                 위로해 드립니다.      0\n",
       "2                 3박4일 놀러가고 싶다               여행은 언제나 좋죠.      0\n",
       "3              3박4일 정도 놀러가고 싶다               여행은 언제나 좋죠.      0\n",
       "4                      PPL 심하네                눈살이 찌푸려지죠.      0\n",
       "...                        ...                       ...    ...\n",
       "11818           훔쳐보는 것도 눈치 보임.        티가 나니까 눈치가 보이는 거죠!      2\n",
       "11819           훔쳐보는 것도 눈치 보임.             훔쳐보는 거 티나나봐요.      2\n",
       "11820              흑기사 해주는 짝남.                    설렜겠어요.      2\n",
       "11821  힘든 연애 좋은 연애라는게 무슨 차이일까?  잘 헤어질 수 있는 사이 여부인 거 같아요.      2\n",
       "11822               힘들어서 결혼할까봐        도피성 결혼은 하지 않길 바라요.      2\n",
       "\n",
       "[11823 rows x 3 columns]"
      ]
     },
     "execution_count": 92,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "id": "424a69f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "def clean_text(text):\n",
    "    text = re.sub(r'[^\\w\\s가-힣]', '', text)  # 특수문자 제거 (영어 포함 가능)\n",
    "    text = re.sub(r'\\s+', ' ', text).strip()  # 공백 제거\n",
    "    return text\n",
    "\n",
    "data['Q'] = data['Q'].apply(clean_text)\n",
    "data['A'] = data['A'].apply(clean_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "id": "e8b3b74e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Q</th>\n",
       "      <th>A</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>12시 땡</td>\n",
       "      <td>하루가 또 가네요</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1지망 학교 떨어졌어</td>\n",
       "      <td>위로해 드립니다</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3박4일 놀러가고 싶다</td>\n",
       "      <td>여행은 언제나 좋죠</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3박4일 정도 놀러가고 싶다</td>\n",
       "      <td>여행은 언제나 좋죠</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>PPL 심하네</td>\n",
       "      <td>눈살이 찌푸려지죠</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11818</th>\n",
       "      <td>훔쳐보는 것도 눈치 보임</td>\n",
       "      <td>티가 나니까 눈치가 보이는 거죠</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11819</th>\n",
       "      <td>훔쳐보는 것도 눈치 보임</td>\n",
       "      <td>훔쳐보는 거 티나나봐요</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11820</th>\n",
       "      <td>흑기사 해주는 짝남</td>\n",
       "      <td>설렜겠어요</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11821</th>\n",
       "      <td>힘든 연애 좋은 연애라는게 무슨 차이일까</td>\n",
       "      <td>잘 헤어질 수 있는 사이 여부인 거 같아요</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11822</th>\n",
       "      <td>힘들어서 결혼할까봐</td>\n",
       "      <td>도피성 결혼은 하지 않길 바라요</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>11823 rows × 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                            Q                        A  label\n",
       "0                       12시 땡                하루가 또 가네요      0\n",
       "1                 1지망 학교 떨어졌어                 위로해 드립니다      0\n",
       "2                3박4일 놀러가고 싶다               여행은 언제나 좋죠      0\n",
       "3             3박4일 정도 놀러가고 싶다               여행은 언제나 좋죠      0\n",
       "4                     PPL 심하네                눈살이 찌푸려지죠      0\n",
       "...                       ...                      ...    ...\n",
       "11818           훔쳐보는 것도 눈치 보임        티가 나니까 눈치가 보이는 거죠      2\n",
       "11819           훔쳐보는 것도 눈치 보임             훔쳐보는 거 티나나봐요      2\n",
       "11820              흑기사 해주는 짝남                    설렜겠어요      2\n",
       "11821  힘든 연애 좋은 연애라는게 무슨 차이일까  잘 헤어질 수 있는 사이 여부인 거 같아요      2\n",
       "11822              힘들어서 결혼할까봐        도피성 결혼은 하지 않길 바라요      2\n",
       "\n",
       "[11823 rows x 3 columns]"
      ]
     },
     "execution_count": 94,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "id": "be01ea84",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vocabulary size: 10125\n"
     ]
    }
   ],
   "source": [
    "# 토크나이징 \n",
    "\n",
    "questions = data['Q'].tolist()\n",
    "answers = data['A'].tolist()\n",
    "\n",
    "tokenizer = tfds.deprecated.text.SubwordTextEncoder.build_from_corpus(\n",
    "    questions + answers,\n",
    "    target_vocab_size = len(data)\n",
    "#     target_vocab_size=2**13\n",
    ")\n",
    "\n",
    "# vocab 확인\n",
    "print(\"Vocabulary size:\", tokenizer.vocab_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "id": "062148c9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "서브워드 집합(vocabulary)의 크기 : 10125\n",
      "등장 빈도가 4번 미만인 희귀 서브워드 수: 2952\n",
      "희귀 서브워드 제외 시 단어 집합의 크기 : 7173\n",
      "서브워드 집합에서 희귀 서브워드 비율: 29.16%\n",
      "전체 등장 빈도에서 희귀 서브워드 비율: 7.90%\n"
     ]
    }
   ],
   "source": [
    "from collections import Counter\n",
    "\n",
    "threshold = 4\n",
    "\n",
    "# 전체 문장 리스트 (이미 정제된 질문 + 답변 리스트)\n",
    "corpus = questions + answers\n",
    "\n",
    "# 서브워드 토큰 등장 횟수 계산\n",
    "token_counter = Counter()\n",
    "for sentence in corpus:\n",
    "    token_ids = tokenizer.encode(sentence)\n",
    "    token_counter.update(token_ids)\n",
    "\n",
    "# 통계 계산\n",
    "total_cnt = tokenizer.vocab_size\n",
    "total_freq = sum(token_counter.values())\n",
    "\n",
    "rare_cnt = 0\n",
    "rare_freq = 0\n",
    "\n",
    "for token_id, freq in token_counter.items():\n",
    "    if freq < threshold:\n",
    "        rare_cnt += 1\n",
    "        rare_freq += freq\n",
    "\n",
    "print('서브워드 집합(vocabulary)의 크기 :', total_cnt)\n",
    "print('등장 빈도가 %d번 미만인 희귀 서브워드 수: %d' % (threshold, rare_cnt))\n",
    "print('희귀 서브워드 제외 시 단어 집합의 크기 : %d' % (total_cnt - rare_cnt))\n",
    "print('서브워드 집합에서 희귀 서브워드 비율: %.2f%%' % ((rare_cnt / total_cnt) * 100))\n",
    "print('전체 등장 빈도에서 희귀 서브워드 비율: %.2f%%' % ((rare_freq / total_freq) * 100))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fa75f51a",
   "metadata": {},
   "source": [
    "희귀 서브워드를 날리는 것보다 사용하기로 선택\n",
    "\n",
    "전체 등장 빈도에서 8%나 차지함"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "id": "8fac41f4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vocabulary size: 10125\n"
     ]
    }
   ],
   "source": [
    "questions = data['Q'].tolist()\n",
    "answers = data['A'].tolist()\n",
    "\n",
    "tokenizer = tfds.deprecated.text.SubwordTextEncoder.build_from_corpus(\n",
    "    questions + answers,\n",
    "    target_vocab_size = len(data)\n",
    ")\n",
    "\n",
    "# vocab 확인\n",
    "print(\"Vocabulary size:\", tokenizer.vocab_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "id": "7c262aef",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "슝=3\n",
      "START_TOKEN의 번호 : [10125]\n",
      "END_TOKEN의 번호 : [10126]\n",
      "10127\n"
     ]
    }
   ],
   "source": [
    "# 시작 토큰과 종료 토큰에 고유한 정수를 부여합니다.\n",
    "START_TOKEN, END_TOKEN = [tokenizer.vocab_size], [tokenizer.vocab_size + 1]\n",
    "print('START_TOKEN의 번호 :' ,[tokenizer.vocab_size])\n",
    "print('END_TOKEN의 번호 :' ,[tokenizer.vocab_size + 1])\n",
    "# 시작 토큰과 종료 토큰을 고려하여 +2를 하여 단어장의 크기를 산정합니다.\n",
    "VOCAB_SIZE = tokenizer.vocab_size + 2\n",
    "print(VOCAB_SIZE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "id": "be45c873",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "정수 인코딩 후의 21번째 질문 샘플: [9808, 2192, 3609]\n",
      "정수 인코딩 후의 21번째 답변 샘플: [2064, 6557, 5, 5462, 123]\n"
     ]
    }
   ],
   "source": [
    "# 임의의 22번째 샘플에 대해서 정수 인코딩 작업을 수행.\n",
    "# 각 토큰을 고유한 정수로 변환\n",
    "print('정수 인코딩 후의 21번째 질문 샘플: {}'.format(tokenizer.encode(questions[21])))\n",
    "print('정수 인코딩 후의 21번째 답변 샘플: {}'.format(tokenizer.encode(answers[21])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "id": "dbf53cfe",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "원본 질문: 12시 땡\n",
      "인코딩된 질문: [10125, 6932, 3006, 4776, 10126]\n",
      "디코딩 확인: 12시 땡\n"
     ]
    }
   ],
   "source": [
    "def encode_sentence(sentence):\n",
    "    return START_TOKEN + tokenizer.encode(sentence) + END_TOKEN\n",
    "\n",
    "# questions와 answers 각각 인코딩\n",
    "questions_encoded = [encode_sentence(sentence) for sentence in questions]\n",
    "answers_encoded = [encode_sentence(sentence) for sentence in answers]\n",
    "\n",
    "# 예시 출력\n",
    "print(\"원본 질문:\", questions[0])\n",
    "print(\"인코딩된 질문:\", questions_encoded[0])\n",
    "print(\"디코딩 확인:\", tokenizer.decode(questions_encoded[0][1:-1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "id": "ab28c5c0",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "문장길이 평균 :  6.730694409202402\n",
      "문장길이 최대 :  28\n",
      "문장길이 표준편차 :  2.3165233036673962\n",
      "pad_sequences maxlen :  13\n",
      "전체 문장의 0.9868899602469763%가 maxlen 설정값 이내에 포함됩니다. \n"
     ]
    }
   ],
   "source": [
    "total_data_text = questions_encoded + answers_encoded\n",
    "# 텍스트데이터 문장길이의 리스트를 생성한 후\n",
    "num_tokens = [len(tokens) for tokens in total_data_text]\n",
    "num_tokens = np.array(num_tokens)\n",
    "# 문장길이의 평균값, 최대값, 표준편차를 계산해 본다.\n",
    "print('문장길이 평균 : ', np.mean(num_tokens))\n",
    "print('문장길이 최대 : ', np.max(num_tokens))\n",
    "print('문장길이 표준편차 : ', np.std(num_tokens))\n",
    "\n",
    "# 예를들어, 최대 길이를 (평균 + 2*표준편차)로 한다면,\n",
    "max_tokens = np.mean(num_tokens) + 3 * np.std(num_tokens)\n",
    "maxlen = int(max_tokens)\n",
    "print('pad_sequences maxlen : ', maxlen)\n",
    "print(f'전체 문장의 {np.sum(num_tokens < max_tokens) / len(num_tokens)}%가 maxlen 설정값 이내에 포함됩니다. ')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "id": "c58c660f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "13\n"
     ]
    }
   ],
   "source": [
    "# 샘플의 최대 허용 길이 또는 패딩 후의 최종 길이\n",
    "MAX_LENGTH = 13\n",
    "print(MAX_LENGTH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "id": "a09d1989",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "슝=3\n"
     ]
    }
   ],
   "source": [
    "# 정수 인코딩, 최대 길이를 초과하는 샘플 제거, 패딩\n",
    "def tokenize_and_filter(inputs, outputs):\n",
    "  tokenized_inputs, tokenized_outputs = [], []\n",
    "\n",
    "  for (sentence1, sentence2) in zip(inputs, outputs):\n",
    "    # 정수 인코딩 과정에서 시작 토큰과 종료 토큰을 추가\n",
    "    sentence1 = START_TOKEN + tokenizer.encode(sentence1) + END_TOKEN\n",
    "    sentence2 = START_TOKEN + tokenizer.encode(sentence2) + END_TOKEN\n",
    "\n",
    "    # 최대 길이 40 이하인 경우에만 데이터셋으로 허용\n",
    "    if len(sentence1) <= MAX_LENGTH and len(sentence2) <= MAX_LENGTH:\n",
    "      tokenized_inputs.append(sentence1)\n",
    "      tokenized_outputs.append(sentence2)\n",
    "\n",
    "  # 최대 길이 40으로 모든 데이터셋을 패딩\n",
    "  tokenized_inputs = tf.keras.preprocessing.sequence.pad_sequences(\n",
    "      tokenized_inputs, maxlen=MAX_LENGTH, padding='post')\n",
    "  tokenized_outputs = tf.keras.preprocessing.sequence.pad_sequences(\n",
    "      tokenized_outputs, maxlen=MAX_LENGTH, padding='post')\n",
    "\n",
    "  return tokenized_inputs, tokenized_outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "id": "8d4884e0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "단어장의 크기 : 10127\n",
      "필터링 후의 질문 샘플 개수: 11516\n",
      "필터링 후의 답변 샘플 개수: 11516\n"
     ]
    }
   ],
   "source": [
    "questions, answers = tokenize_and_filter(questions, answers)\n",
    "print('단어장의 크기 :',(VOCAB_SIZE))\n",
    "print('필터링 후의 질문 샘플 개수: {}'.format(len(questions)))\n",
    "print('필터링 후의 답변 샘플 개수: {}'.format(len(answers)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "id": "4409bc1d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "슝=3\n"
     ]
    }
   ],
   "source": [
    "BATCH_SIZE = 64\n",
    "BUFFER_SIZE = 20000\n",
    "\n",
    "# 디코더는 이전의 target을 다음의 input으로 사용합니다.\n",
    "# 이에 따라 outputs에서는 START_TOKEN을 제거하겠습니다.\n",
    "dataset = tf.data.Dataset.from_tensor_slices((\n",
    "    {\n",
    "        'inputs': questions,\n",
    "        'dec_inputs': answers[:, :-1]\n",
    "    },\n",
    "    {\n",
    "        'outputs': answers[:, 1:]\n",
    "    },\n",
    "))\n",
    "\n",
    "# 배치 데이터셋을 생성\n",
    "dataset = dataset.cache()\n",
    "dataset = dataset.shuffle(BUFFER_SIZE)\n",
    "dataset = dataset.batch(BATCH_SIZE)\n",
    "dataset = dataset.prefetch(tf.data.experimental.AUTOTUNE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "id": "29ebd8f9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "슝=3\n"
     ]
    }
   ],
   "source": [
    "def transformer(vocab_size,\n",
    "                num_layers,\n",
    "                units,\n",
    "                d_model,\n",
    "                num_heads,\n",
    "                dropout,\n",
    "                name=\"transformer\"):\n",
    "  inputs = tf.keras.Input(shape=(None,), name=\"inputs\")\n",
    "  dec_inputs = tf.keras.Input(shape=(None,), name=\"dec_inputs\")\n",
    "\n",
    "  # 인코더에서 패딩을 위한 마스크\n",
    "  enc_padding_mask = tf.keras.layers.Lambda(\n",
    "      create_padding_mask, output_shape=(1, 1, None),\n",
    "      name='enc_padding_mask')(inputs)\n",
    "\n",
    "  # 디코더에서 미래의 토큰을 마스크 하기 위해서 사용합니다.\n",
    "  # 내부적으로 패딩 마스크도 포함되어져 있습니다.\n",
    "  look_ahead_mask = tf.keras.layers.Lambda(\n",
    "      create_look_ahead_mask,\n",
    "      output_shape=(1, None, None),\n",
    "      name='look_ahead_mask')(dec_inputs)\n",
    "\n",
    "  # 두 번째 어텐션 블록에서 인코더의 벡터들을 마스킹\n",
    "  # 디코더에서 패딩을 위한 마스크\n",
    "  dec_padding_mask = tf.keras.layers.Lambda(\n",
    "      create_padding_mask, output_shape=(1, 1, None),\n",
    "      name='dec_padding_mask')(inputs)\n",
    "\n",
    "  # 인코더\n",
    "  enc_outputs = encoder(\n",
    "      vocab_size=vocab_size,\n",
    "      num_layers=num_layers,\n",
    "      units=units,\n",
    "      d_model=d_model,\n",
    "      num_heads=num_heads,\n",
    "      dropout=dropout,\n",
    "  )(inputs=[inputs, enc_padding_mask])\n",
    "\n",
    "  # 디코더\n",
    "  dec_outputs = decoder(\n",
    "      vocab_size=vocab_size,\n",
    "      num_layers=num_layers,\n",
    "      units=units,\n",
    "      d_model=d_model,\n",
    "      num_heads=num_heads,\n",
    "      dropout=dropout,\n",
    "  )(inputs=[dec_inputs, enc_outputs, look_ahead_mask, dec_padding_mask])\n",
    "\n",
    "  # 완전연결층\n",
    "  outputs = tf.keras.layers.Dense(units=vocab_size, name=\"outputs\")(dec_outputs)\n",
    "\n",
    "  return tf.keras.Model(inputs=[inputs, dec_inputs], outputs=outputs, name=name)\n",
    "print(\"슝=3\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "id": "0fef8bf3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"transformer\"\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "inputs (InputLayer)             [(None, None)]       0                                            \n",
      "__________________________________________________________________________________________________\n",
      "dec_inputs (InputLayer)         [(None, None)]       0                                            \n",
      "__________________________________________________________________________________________________\n",
      "enc_padding_mask (Lambda)       (None, 1, 1, None)   0           inputs[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "encoder (Functional)            (None, None, 256)    5755136     inputs[0][0]                     \n",
      "                                                                 enc_padding_mask[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "look_ahead_mask (Lambda)        (None, 1, None, None 0           dec_inputs[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "dec_padding_mask (Lambda)       (None, 1, 1, None)   0           inputs[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "decoder (Functional)            (None, None, 256)    7337216     dec_inputs[0][0]                 \n",
      "                                                                 encoder[0][0]                    \n",
      "                                                                 look_ahead_mask[0][0]            \n",
      "                                                                 dec_padding_mask[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "outputs (Dense)                 (None, None, 10127)  2602639     decoder[0][0]                    \n",
      "==================================================================================================\n",
      "Total params: 15,694,991\n",
      "Trainable params: 15,694,991\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "tf.keras.backend.clear_session()\n",
    "\n",
    "# 하이퍼파라미터\n",
    "NUM_LAYERS = 6 # 인코더와 디코더의 층의 개수\n",
    "D_MODEL = 256 # 인코더와 디코더 내부의 입, 출력의 고정 차원\n",
    "NUM_HEADS = 8 # 멀티 헤드 어텐션에서의 헤드 수\n",
    "UNITS = 512 # 피드 포워드 신경망의 은닉층의 크기\n",
    "DROPOUT = 0.1 # 드롭아웃의 비율\n",
    "\n",
    "model = transformer(\n",
    "    vocab_size=VOCAB_SIZE,\n",
    "    num_layers=NUM_LAYERS,\n",
    "    units=UNITS,\n",
    "    d_model=D_MODEL,\n",
    "    num_heads=NUM_HEADS,\n",
    "    dropout=DROPOUT)\n",
    "\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "id": "12a21aa3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "슝=3\n"
     ]
    }
   ],
   "source": [
    "def loss_function(y_true, y_pred):\n",
    "  y_true = tf.reshape(y_true, shape=(-1, MAX_LENGTH - 1))\n",
    "\n",
    "  loss = tf.keras.losses.SparseCategoricalCrossentropy(\n",
    "      from_logits=True, reduction='none')(y_true, y_pred)\n",
    "\n",
    "  mask = tf.cast(tf.not_equal(y_true, 0), tf.float32)\n",
    "  loss = tf.multiply(loss, mask)\n",
    "\n",
    "  return tf.reduce_mean(loss)\n",
    "print(\"슝=3\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "id": "cea7d07b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "슝=3\n"
     ]
    }
   ],
   "source": [
    "class CustomSchedule(tf.keras.optimizers.schedules.LearningRateSchedule):\n",
    "\n",
    "  def __init__(self, d_model, warmup_steps=4000):\n",
    "    super(CustomSchedule, self).__init__()\n",
    "\n",
    "    self.d_model = d_model\n",
    "    self.d_model = tf.cast(self.d_model, tf.float32)\n",
    "\n",
    "    self.warmup_steps = warmup_steps\n",
    "\n",
    "  def __call__(self, step):\n",
    "    arg1 = tf.math.rsqrt(step)\n",
    "    arg2 = step * (self.warmup_steps**-1.5)\n",
    "\n",
    "    return tf.math.rsqrt(self.d_model) * tf.math.minimum(arg1, arg2)\n",
    "print(\"슝=3\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "id": "8ff19c5d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "슝=3\n"
     ]
    }
   ],
   "source": [
    "learning_rate = CustomSchedule(D_MODEL)\n",
    "\n",
    "optimizer = tf.keras.optimizers.Adam(\n",
    "    learning_rate, beta_1=0.9, beta_2=0.98, epsilon=1e-9)\n",
    "\n",
    "def accuracy(y_true, y_pred):\n",
    "  y_true = tf.reshape(y_true, shape=(-1, MAX_LENGTH - 1))\n",
    "  return tf.keras.metrics.sparse_categorical_accuracy(y_true, y_pred)\n",
    "\n",
    "model.compile(optimizer=optimizer, loss=loss_function, metrics=[accuracy])\n",
    "print(\"슝=3\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "id": "3670a920",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/50\n",
      "180/180 [==============================] - 27s 68ms/step - loss: 3.8804 - accuracy: 0.0737\n",
      "Epoch 2/50\n",
      "180/180 [==============================] - 12s 67ms/step - loss: 3.3662 - accuracy: 0.0833\n",
      "Epoch 3/50\n",
      "180/180 [==============================] - 12s 67ms/step - loss: 3.0636 - accuracy: 0.0835\n",
      "Epoch 4/50\n",
      "180/180 [==============================] - 12s 67ms/step - loss: 2.8927 - accuracy: 0.0863\n",
      "Epoch 5/50\n",
      "180/180 [==============================] - 12s 67ms/step - loss: 2.7845 - accuracy: 0.0911\n",
      "Epoch 6/50\n",
      "180/180 [==============================] - 12s 67ms/step - loss: 2.7043 - accuracy: 0.0949\n",
      "Epoch 7/50\n",
      "180/180 [==============================] - 12s 67ms/step - loss: 2.6131 - accuracy: 0.1010\n",
      "Epoch 8/50\n",
      "180/180 [==============================] - 12s 67ms/step - loss: 2.5163 - accuracy: 0.1062\n",
      "Epoch 9/50\n",
      "180/180 [==============================] - 12s 66ms/step - loss: 2.4182 - accuracy: 0.1113\n",
      "Epoch 10/50\n",
      "180/180 [==============================] - 12s 67ms/step - loss: 2.3094 - accuracy: 0.1185\n",
      "Epoch 11/50\n",
      "180/180 [==============================] - 12s 67ms/step - loss: 2.1922 - accuracy: 0.1271\n",
      "Epoch 12/50\n",
      "180/180 [==============================] - 12s 67ms/step - loss: 2.0688 - accuracy: 0.1368\n",
      "Epoch 13/50\n",
      "180/180 [==============================] - 12s 67ms/step - loss: 1.9376 - accuracy: 0.1484\n",
      "Epoch 14/50\n",
      "180/180 [==============================] - 12s 67ms/step - loss: 1.8053 - accuracy: 0.1617\n",
      "Epoch 15/50\n",
      "180/180 [==============================] - 12s 67ms/step - loss: 1.6697 - accuracy: 0.1770\n",
      "Epoch 16/50\n",
      "180/180 [==============================] - 12s 67ms/step - loss: 1.5343 - accuracy: 0.1929\n",
      "Epoch 17/50\n",
      "180/180 [==============================] - 13s 71ms/step - loss: 1.4143 - accuracy: 0.2094\n",
      "Epoch 18/50\n",
      "180/180 [==============================] - 14s 80ms/step - loss: 1.3042 - accuracy: 0.2243\n",
      "Epoch 19/50\n",
      "180/180 [==============================] - 14s 79ms/step - loss: 1.2037 - accuracy: 0.2381\n",
      "Epoch 20/50\n",
      "180/180 [==============================] - 23s 127ms/step - loss: 1.1239 - accuracy: 0.2486\n",
      "Epoch 21/50\n",
      "180/180 [==============================] - 26s 142ms/step - loss: 1.0641 - accuracy: 0.2558\n",
      "Epoch 22/50\n",
      "180/180 [==============================] - 27s 149ms/step - loss: 1.0133 - accuracy: 0.2639\n",
      "Epoch 23/50\n",
      "180/180 [==============================] - 27s 152ms/step - loss: 0.9690 - accuracy: 0.2716\n",
      "Epoch 24/50\n",
      "180/180 [==============================] - 27s 151ms/step - loss: 0.9199 - accuracy: 0.2796\n",
      "Epoch 25/50\n",
      "180/180 [==============================] - 18s 100ms/step - loss: 0.8704 - accuracy: 0.2891\n",
      "Epoch 26/50\n",
      "180/180 [==============================] - 13s 75ms/step - loss: 0.8342 - accuracy: 0.2941\n",
      "Epoch 27/50\n",
      "180/180 [==============================] - 13s 74ms/step - loss: 0.8043 - accuracy: 0.3005\n",
      "Epoch 28/50\n",
      "180/180 [==============================] - 12s 67ms/step - loss: 0.7758 - accuracy: 0.3049\n",
      "Epoch 29/50\n",
      "180/180 [==============================] - 12s 67ms/step - loss: 0.7546 - accuracy: 0.3085\n",
      "Epoch 30/50\n",
      "180/180 [==============================] - 12s 67ms/step - loss: 0.7338 - accuracy: 0.3111\n",
      "Epoch 31/50\n",
      "180/180 [==============================] - 12s 67ms/step - loss: 0.7124 - accuracy: 0.3141\n",
      "Epoch 32/50\n",
      "180/180 [==============================] - 12s 67ms/step - loss: 0.6959 - accuracy: 0.3174\n",
      "Epoch 33/50\n",
      "180/180 [==============================] - 12s 68ms/step - loss: 0.6808 - accuracy: 0.3194\n",
      "Epoch 34/50\n",
      "180/180 [==============================] - 12s 67ms/step - loss: 0.6656 - accuracy: 0.3214\n",
      "Epoch 35/50\n",
      "180/180 [==============================] - 12s 67ms/step - loss: 0.6494 - accuracy: 0.3239\n",
      "Epoch 36/50\n",
      "180/180 [==============================] - 12s 67ms/step - loss: 0.6392 - accuracy: 0.3247\n",
      "Epoch 37/50\n",
      "180/180 [==============================] - 12s 67ms/step - loss: 0.6260 - accuracy: 0.3267\n",
      "Epoch 38/50\n",
      "180/180 [==============================] - 12s 68ms/step - loss: 0.6126 - accuracy: 0.3283\n",
      "Epoch 39/50\n",
      "180/180 [==============================] - 12s 67ms/step - loss: 0.6032 - accuracy: 0.3294\n",
      "Epoch 40/50\n",
      "180/180 [==============================] - 12s 67ms/step - loss: 0.5948 - accuracy: 0.3304\n",
      "Epoch 41/50\n",
      "180/180 [==============================] - 12s 67ms/step - loss: 0.5873 - accuracy: 0.3318\n",
      "Epoch 42/50\n",
      "180/180 [==============================] - 12s 67ms/step - loss: 0.5719 - accuracy: 0.3341\n",
      "Epoch 43/50\n",
      "180/180 [==============================] - 12s 68ms/step - loss: 0.5669 - accuracy: 0.3338\n",
      "Epoch 44/50\n",
      "180/180 [==============================] - 12s 67ms/step - loss: 0.5594 - accuracy: 0.3348\n",
      "Epoch 45/50\n",
      "180/180 [==============================] - 12s 67ms/step - loss: 0.5477 - accuracy: 0.3368\n",
      "Epoch 46/50\n",
      "180/180 [==============================] - 12s 67ms/step - loss: 0.5463 - accuracy: 0.3366\n",
      "Epoch 47/50\n",
      "180/180 [==============================] - 12s 66ms/step - loss: 0.5394 - accuracy: 0.3388\n",
      "Epoch 48/50\n",
      "180/180 [==============================] - 12s 67ms/step - loss: 0.5305 - accuracy: 0.3395\n",
      "Epoch 49/50\n",
      "180/180 [==============================] - 12s 67ms/step - loss: 0.5231 - accuracy: 0.3397\n",
      "Epoch 50/50\n",
      "180/180 [==============================] - 12s 67ms/step - loss: 0.5179 - accuracy: 0.3407\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x77a26c78c550>"
      ]
     },
     "execution_count": 134,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "EPOCHS = 50\n",
    "model.fit(dataset, epochs=EPOCHS, verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "id": "8b8e24cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "# 예측 함수\n",
    "def predict_answer(question, tokenizer, model, max_length=13):\n",
    "    # 질문을 토큰화\n",
    "    question_encoded = tokenizer.encode(question)\n",
    "    question_padded = tf.keras.preprocessing.sequence.pad_sequences([question_encoded], maxlen=max_length, padding='post')\n",
    "\n",
    "    # 디코더 입력은 시작 토큰으로 초기화\n",
    "    start_token = tokenizer.vocab_size  # 시작 토큰 <START_TOKEN>\n",
    "    dec_input = tf.constant([[start_token]], dtype=tf.int64)  # <START_TOKEN> 입력\n",
    "\n",
    "    # 예측 시작\n",
    "    for _ in range(max_length - 1):  # 최대 길이 (13까지 예측)\n",
    "        # 예측을 얻는다 (모델에 입력)\n",
    "        prediction = model([question_padded, dec_input])\n",
    "\n",
    "        # 예측된 값 중 가장 확률이 높은 토큰을 선택\n",
    "        predicted_token = tf.argmax(prediction, axis=-1)[:, -1].numpy()\n",
    "\n",
    "        # 종료 토큰 <END_TOKEN>이 나오면 예측을 멈춘다\n",
    "        if predicted_token == tokenizer.vocab_size + 1:  # <END_TOKEN>\n",
    "            break\n",
    "\n",
    "        # 예측된 토큰을 [1, 1] 형태로 변환하여 디코더 입력에 추가\n",
    "        predicted_token = tf.reshape(predicted_token, (1, 1))  # 차원 맞추기\n",
    "        predicted_token = tf.cast(predicted_token, tf.int64)  # 데이터 타입을 int64로 맞추기\n",
    "        dec_input = tf.concat([dec_input, predicted_token], axis=-1)\n",
    "\n",
    "    # 예측된 토큰을 디코딩하여 답변 생성\n",
    "    predicted_answer = tokenizer.decode(dec_input.numpy()[0][1:])  # 시작 토큰 제외\n",
    "    return predicted_answer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 226,
   "id": "9cc7c81a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "265                        괜찮은 사람인데 사귀긴 싫어\n",
       "488                          나 몰래 사귀는 거 같애\n",
       "1350                           둘이 사귀는 거 같애\n",
       "1507                        만나면 좋은데 사귀긴 싫어\n",
       "2257                                사귀고 싶어\n",
       "                       ...                \n",
       "11503    짝녀가 사귀던 사람이랑 헤어졌는데 좋아한다고 고백해도 되나.\n",
       "11692              친구도 아니고 사귀는 사이도 아닌데 뭐지?\n",
       "11718               친한 친구의 구남친이랑 사귀어도 될까요?\n",
       "11719                친한 친구의 구여친이랑 사귀어도 되나.\n",
       "11739               클럽에서 만나서 사귀는거 어떻게 생각해?\n",
       "Name: Q, Length: 70, dtype: object"
      ]
     },
     "execution_count": 226,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_row['Q'][data_row['Q'].str.contains('사귀')]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "id": "8ba2f91b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "질문: 마 나랑 사귀자 내가 잘해줄게\n",
      "예측된 답변: 자신을 더 사랑해주세요\n"
     ]
    }
   ],
   "source": [
    "# 예시로 질문 넣기\n",
    "input_question = \"마 나랑 사귀자 내가 잘해줄게\"\n",
    "answer = predict_answer(input_question, tokenizer, model)\n",
    "\n",
    "# 결과 출력\n",
    "print(\"질문:\", input_question)\n",
    "print(\"예측된 답변:\", answer)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 202,
   "id": "a6bba33a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "질문: 나랑 사귀자 내가 잘해줄게\n",
      "예측된 답변: 이제 같은 실수 안 하면 돼요\n"
     ]
    }
   ],
   "source": [
    "# 예시로 질문 넣기\n",
    "input_question = \"나랑 사귀자 내가 잘해줄게\"\n",
    "answer = predict_answer(input_question, tokenizer, model)\n",
    "\n",
    "# 결과 출력\n",
    "print(\"질문:\", input_question)\n",
    "print(\"예측된 답변:\", answer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "id": "6aa5e52f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "질문: 자살 생각이 들어..\n",
      "예측된 답변: 마음 먹은 것만으로도 절반을 해낸 거예요\n"
     ]
    }
   ],
   "source": [
    "# 예시로 질문 넣기\n",
    "input_question = \"자살 생각이 들어..\"\n",
    "answer = predict_answer(input_question, tokenizer, model)\n",
    "\n",
    "# 결과 출력\n",
    "print(\"질문:\", input_question)\n",
    "print(\"예측된 답변:\", answer)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "id": "bc87dc1e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "질문: 너 미쳤어? 나보고 자살하라는거지?\n",
      "예측된 답변: 마음이 복잡한가봐요\n"
     ]
    }
   ],
   "source": [
    "# 예시로 질문 넣기\n",
    "input_question = \"너 미쳤어? 나보고 자살하라는거지?\"\n",
    "answer = predict_answer(input_question, tokenizer, model)\n",
    "\n",
    "# 결과 출력\n",
    "print(\"질문:\", input_question)\n",
    "print(\"예측된 답변:\", answer)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 215,
   "id": "cfe90f46",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "질문: 인생이란 뭘까\n",
      "예측된 답변: 스트레스 받지 마세요\n"
     ]
    }
   ],
   "source": [
    "# 예시로 질문 넣기\n",
    "input_question = \"인생이란 뭘까\"\n",
    "answer = predict_answer(input_question, tokenizer, model)\n",
    "\n",
    "# 결과 출력\n",
    "print(\"질문:\", input_question)\n",
    "print(\"예측된 답변:\", answer)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f32655d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 216,
   "id": "d561dfb3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "질문: 피곤하네\n",
      "예측된 답변: 아무래도 그렇겠죠\n"
     ]
    }
   ],
   "source": [
    "# 예시로 질문 넣기\n",
    "input_question = \"피곤하네\"\n",
    "answer = predict_answer(input_question, tokenizer, model)\n",
    "\n",
    "# 결과 출력\n",
    "print(\"질문:\", input_question)\n",
    "print(\"예측된 답변:\", answer)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "69a2d1b4",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad2986a4",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "96fe41e6",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "7f3d54eb",
   "metadata": {},
   "source": [
    "1. train valid 나눠서 일반화 성능을 올려보자\n",
    "\n",
    "2. 인코더 디코더 레이어를 낮추고 입출력 차원, dense 차원을 낮춰보자"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 227,
   "id": "eb0c1939",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training dataset size: 9212\n",
      "Validation dataset size: 2304\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "# 전체 데이터셋을 80% train, 20% validation으로 나누기\n",
    "TRAIN_RATIO = 0.8\n",
    "VAL_RATIO = 0.2\n",
    "\n",
    "# 데이터셋 크기\n",
    "dataset_size = len(questions)\n",
    "\n",
    "# 훈련 데이터와 검증 데이터의 인덱스 크기\n",
    "train_size = int(TRAIN_RATIO * dataset_size)\n",
    "val_size = dataset_size - train_size\n",
    "\n",
    "# 데이터셋을 shuffle한 뒤, 나누기\n",
    "dataset = tf.data.Dataset.from_tensor_slices(( \n",
    "    {\n",
    "        'inputs': questions,\n",
    "        'dec_inputs': answers[:, :-1]\n",
    "    },\n",
    "    {\n",
    "        'outputs': answers[:, 1:]\n",
    "    },\n",
    "))\n",
    "\n",
    "# 데이터셋을 섞고 배치처리\n",
    "dataset = dataset.cache()\n",
    "dataset = dataset.shuffle(BUFFER_SIZE)\n",
    "dataset = dataset.batch(BATCH_SIZE)\n",
    "\n",
    "# 전체 데이터셋을 80%는 훈련용, 20%는 검증용으로 분할\n",
    "train_dataset = dataset.take(train_size // BATCH_SIZE)\n",
    "val_dataset = dataset.skip(train_size // BATCH_SIZE)\n",
    "\n",
    "# 각 데이터셋을 미리 처리하기 위한 Prefetch\n",
    "train_dataset = train_dataset.prefetch(tf.data.experimental.AUTOTUNE)\n",
    "val_dataset = val_dataset.prefetch(tf.data.experimental.AUTOTUNE)\n",
    "\n",
    "# 데이터셋 확인\n",
    "print(f'Training dataset size: {train_size}')\n",
    "print(f'Validation dataset size: {val_size}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 228,
   "id": "05b9adde",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"transformer\"\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "inputs (InputLayer)             [(None, None)]       0                                            \n",
      "__________________________________________________________________________________________________\n",
      "dec_inputs (InputLayer)         [(None, None)]       0                                            \n",
      "__________________________________________________________________________________________________\n",
      "enc_padding_mask (Lambda)       (None, 1, 1, None)   0           inputs[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "encoder (Functional)            (None, None, 128)    1826176     inputs[0][0]                     \n",
      "                                                                 enc_padding_mask[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "look_ahead_mask (Lambda)        (None, 1, None, None 0           dec_inputs[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "dec_padding_mask (Lambda)       (None, 1, 1, None)   0           inputs[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "decoder (Functional)            (None, None, 128)    2091392     dec_inputs[0][0]                 \n",
      "                                                                 encoder[0][0]                    \n",
      "                                                                 look_ahead_mask[0][0]            \n",
      "                                                                 dec_padding_mask[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "outputs (Dense)                 (None, None, 10127)  1306383     decoder[0][0]                    \n",
      "==================================================================================================\n",
      "Total params: 5,223,951\n",
      "Trainable params: 5,223,951\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "tf.keras.backend.clear_session()\n",
    "\n",
    "# 하이퍼파라미터\n",
    "NUM_LAYERS = 4 # 인코더와 디코더의 층의 개수\n",
    "D_MODEL = 128 # 인코더와 디코더 내부의 입, 출력의 고정 차원\n",
    "NUM_HEADS = 4 # 멀티 헤드 어텐션에서의 헤드 수\n",
    "UNITS = 256 # 피드 포워드 신경망의 은닉층의 크기\n",
    "DROPOUT = 0.1 # 드롭아웃의 비율\n",
    "\n",
    "model2 = transformer(\n",
    "    vocab_size=VOCAB_SIZE,\n",
    "    num_layers=NUM_LAYERS,\n",
    "    units=UNITS,\n",
    "    d_model=D_MODEL,\n",
    "    num_heads=NUM_HEADS,\n",
    "    dropout=DROPOUT)\n",
    "\n",
    "model2.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 229,
   "id": "106528f9",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "143/143 [==============================] - 19s 66ms/step - loss: 3.3279 - accuracy: 0.0827 - val_loss: 3.0944 - val_accuracy: 0.0833\n",
      "Epoch 2/100\n",
      "143/143 [==============================] - 7s 52ms/step - loss: 2.9764 - accuracy: 0.0843 - val_loss: 2.8496 - val_accuracy: 0.0877\n",
      "Epoch 3/100\n",
      "143/143 [==============================] - 7s 52ms/step - loss: 2.7990 - accuracy: 0.0887 - val_loss: 2.6976 - val_accuracy: 0.0915\n",
      "Epoch 4/100\n",
      "143/143 [==============================] - 7s 52ms/step - loss: 2.7184 - accuracy: 0.0938 - val_loss: 2.6363 - val_accuracy: 0.0976\n",
      "Epoch 5/100\n",
      "143/143 [==============================] - 8s 53ms/step - loss: 2.6365 - accuracy: 0.0987 - val_loss: 2.5491 - val_accuracy: 0.1035\n",
      "Epoch 6/100\n",
      "143/143 [==============================] - 8s 53ms/step - loss: 2.5311 - accuracy: 0.1042 - val_loss: 2.4387 - val_accuracy: 0.1085\n",
      "Epoch 7/100\n",
      "143/143 [==============================] - 7s 52ms/step - loss: 2.4377 - accuracy: 0.1101 - val_loss: 2.3097 - val_accuracy: 0.1162\n",
      "Epoch 8/100\n",
      "143/143 [==============================] - 7s 52ms/step - loss: 2.3308 - accuracy: 0.1169 - val_loss: 2.2483 - val_accuracy: 0.1280\n",
      "Epoch 9/100\n",
      "143/143 [==============================] - 7s 52ms/step - loss: 2.2320 - accuracy: 0.1249 - val_loss: 2.1006 - val_accuracy: 0.1378\n",
      "Epoch 10/100\n",
      "143/143 [==============================] - 7s 52ms/step - loss: 2.1221 - accuracy: 0.1338 - val_loss: 1.9525 - val_accuracy: 0.1473\n",
      "Epoch 11/100\n",
      "143/143 [==============================] - 8s 53ms/step - loss: 2.0210 - accuracy: 0.1420 - val_loss: 1.8703 - val_accuracy: 0.1590\n",
      "Epoch 12/100\n",
      "143/143 [==============================] - 8s 53ms/step - loss: 1.9073 - accuracy: 0.1532 - val_loss: 1.7001 - val_accuracy: 0.1770\n",
      "Epoch 13/100\n",
      "143/143 [==============================] - 8s 53ms/step - loss: 1.8034 - accuracy: 0.1651 - val_loss: 1.6129 - val_accuracy: 0.1924\n",
      "Epoch 14/100\n",
      "143/143 [==============================] - 7s 52ms/step - loss: 1.6970 - accuracy: 0.1773 - val_loss: 1.5144 - val_accuracy: 0.2038\n",
      "Epoch 15/100\n",
      "143/143 [==============================] - 7s 52ms/step - loss: 1.5909 - accuracy: 0.1894 - val_loss: 1.3800 - val_accuracy: 0.2230\n",
      "Epoch 16/100\n",
      "143/143 [==============================] - 8s 52ms/step - loss: 1.4974 - accuracy: 0.2016 - val_loss: 1.2802 - val_accuracy: 0.2343\n",
      "Epoch 17/100\n",
      "143/143 [==============================] - 7s 52ms/step - loss: 1.3935 - accuracy: 0.2155 - val_loss: 1.1665 - val_accuracy: 0.2492\n",
      "Epoch 18/100\n",
      "143/143 [==============================] - 8s 53ms/step - loss: 1.3040 - accuracy: 0.2283 - val_loss: 1.0701 - val_accuracy: 0.2712\n",
      "Epoch 19/100\n",
      "143/143 [==============================] - 8s 53ms/step - loss: 1.2212 - accuracy: 0.2414 - val_loss: 0.9680 - val_accuracy: 0.2816\n",
      "Epoch 20/100\n",
      "143/143 [==============================] - 7s 52ms/step - loss: 1.1345 - accuracy: 0.2526 - val_loss: 0.9215 - val_accuracy: 0.2910\n",
      "Epoch 21/100\n",
      "143/143 [==============================] - 8s 52ms/step - loss: 1.0500 - accuracy: 0.2649 - val_loss: 0.8279 - val_accuracy: 0.3074\n",
      "Epoch 22/100\n",
      "143/143 [==============================] - 8s 53ms/step - loss: 0.9790 - accuracy: 0.2769 - val_loss: 0.7371 - val_accuracy: 0.3112\n",
      "Epoch 23/100\n",
      "143/143 [==============================] - 8s 53ms/step - loss: 0.9039 - accuracy: 0.2871 - val_loss: 0.6616 - val_accuracy: 0.3315\n",
      "Epoch 24/100\n",
      "143/143 [==============================] - 8s 53ms/step - loss: 0.8424 - accuracy: 0.2960 - val_loss: 0.6282 - val_accuracy: 0.3301\n",
      "Epoch 25/100\n",
      "143/143 [==============================] - 7s 52ms/step - loss: 0.7754 - accuracy: 0.3039 - val_loss: 0.5598 - val_accuracy: 0.3461\n",
      "Epoch 26/100\n",
      "143/143 [==============================] - 7s 52ms/step - loss: 0.7183 - accuracy: 0.3138 - val_loss: 0.5204 - val_accuracy: 0.3578\n",
      "Epoch 27/100\n",
      "143/143 [==============================] - 8s 52ms/step - loss: 0.6676 - accuracy: 0.3222 - val_loss: 0.4653 - val_accuracy: 0.3639\n",
      "Epoch 28/100\n",
      "143/143 [==============================] - 7s 52ms/step - loss: 0.6169 - accuracy: 0.3318 - val_loss: 0.3908 - val_accuracy: 0.3701\n",
      "Epoch 29/100\n",
      "143/143 [==============================] - 7s 52ms/step - loss: 0.5742 - accuracy: 0.3391 - val_loss: 0.3764 - val_accuracy: 0.3826\n",
      "Epoch 30/100\n",
      "143/143 [==============================] - 7s 52ms/step - loss: 0.5270 - accuracy: 0.3460 - val_loss: 0.3383 - val_accuracy: 0.3822\n",
      "Epoch 31/100\n",
      "143/143 [==============================] - 7s 52ms/step - loss: 0.4940 - accuracy: 0.3548 - val_loss: 0.3074 - val_accuracy: 0.4026\n",
      "Epoch 32/100\n",
      "143/143 [==============================] - 7s 52ms/step - loss: 0.4601 - accuracy: 0.3591 - val_loss: 0.2780 - val_accuracy: 0.4056\n",
      "Epoch 33/100\n",
      "143/143 [==============================] - 8s 53ms/step - loss: 0.4227 - accuracy: 0.3646 - val_loss: 0.2520 - val_accuracy: 0.4094\n",
      "Epoch 34/100\n",
      "143/143 [==============================] - 8s 53ms/step - loss: 0.3915 - accuracy: 0.3722 - val_loss: 0.2210 - val_accuracy: 0.4052\n",
      "Epoch 35/100\n",
      "143/143 [==============================] - 7s 52ms/step - loss: 0.3679 - accuracy: 0.3764 - val_loss: 0.2002 - val_accuracy: 0.4155\n",
      "Epoch 36/100\n",
      "143/143 [==============================] - 7s 52ms/step - loss: 0.3377 - accuracy: 0.3822 - val_loss: 0.1786 - val_accuracy: 0.4224\n",
      "Epoch 37/100\n",
      "143/143 [==============================] - 7s 52ms/step - loss: 0.3143 - accuracy: 0.3865 - val_loss: 0.1594 - val_accuracy: 0.4252\n",
      "Epoch 38/100\n",
      "143/143 [==============================] - 7s 52ms/step - loss: 0.2891 - accuracy: 0.3920 - val_loss: 0.1513 - val_accuracy: 0.4258\n",
      "Epoch 39/100\n",
      "143/143 [==============================] - 7s 52ms/step - loss: 0.2692 - accuracy: 0.3946 - val_loss: 0.1318 - val_accuracy: 0.4297\n",
      "Epoch 40/100\n",
      "143/143 [==============================] - 7s 52ms/step - loss: 0.2485 - accuracy: 0.3987 - val_loss: 0.1165 - val_accuracy: 0.4294\n",
      "Epoch 41/100\n",
      "143/143 [==============================] - 8s 53ms/step - loss: 0.2308 - accuracy: 0.4035 - val_loss: 0.1047 - val_accuracy: 0.4357\n",
      "Epoch 42/100\n",
      "143/143 [==============================] - 7s 52ms/step - loss: 0.2166 - accuracy: 0.4068 - val_loss: 0.0929 - val_accuracy: 0.4333\n",
      "Epoch 43/100\n",
      "143/143 [==============================] - 7s 52ms/step - loss: 0.2050 - accuracy: 0.4091 - val_loss: 0.0824 - val_accuracy: 0.4359\n",
      "Epoch 44/100\n",
      "143/143 [==============================] - 7s 52ms/step - loss: 0.1868 - accuracy: 0.4122 - val_loss: 0.0776 - val_accuracy: 0.4378\n",
      "Epoch 45/100\n",
      "143/143 [==============================] - 7s 52ms/step - loss: 0.1746 - accuracy: 0.4146 - val_loss: 0.0696 - val_accuracy: 0.4387\n",
      "Epoch 46/100\n",
      "143/143 [==============================] - 8s 53ms/step - loss: 0.1644 - accuracy: 0.4183 - val_loss: 0.0593 - val_accuracy: 0.4423\n",
      "Epoch 47/100\n",
      "143/143 [==============================] - 8s 54ms/step - loss: 0.1523 - accuracy: 0.4190 - val_loss: 0.0509 - val_accuracy: 0.4452\n",
      "Epoch 48/100\n",
      "143/143 [==============================] - 8s 52ms/step - loss: 0.1432 - accuracy: 0.4207 - val_loss: 0.0489 - val_accuracy: 0.4417\n",
      "Epoch 49/100\n",
      "143/143 [==============================] - 7s 52ms/step - loss: 0.1341 - accuracy: 0.4241 - val_loss: 0.0425 - val_accuracy: 0.4487\n",
      "Epoch 50/100\n",
      "143/143 [==============================] - 8s 53ms/step - loss: 0.1243 - accuracy: 0.4258 - val_loss: 0.0431 - val_accuracy: 0.4387\n",
      "Epoch 51/100\n",
      "143/143 [==============================] - 7s 52ms/step - loss: 0.1187 - accuracy: 0.4276 - val_loss: 0.0371 - val_accuracy: 0.4429\n",
      "Epoch 52/100\n",
      "143/143 [==============================] - 7s 52ms/step - loss: 0.1103 - accuracy: 0.4296 - val_loss: 0.0341 - val_accuracy: 0.4438\n",
      "Epoch 53/100\n",
      "143/143 [==============================] - 7s 52ms/step - loss: 0.1054 - accuracy: 0.4294 - val_loss: 0.0308 - val_accuracy: 0.4475\n",
      "Epoch 54/100\n",
      "143/143 [==============================] - 8s 53ms/step - loss: 0.0995 - accuracy: 0.4297 - val_loss: 0.0282 - val_accuracy: 0.4520\n",
      "Epoch 55/100\n",
      "143/143 [==============================] - 8s 54ms/step - loss: 0.0949 - accuracy: 0.4324 - val_loss: 0.0277 - val_accuracy: 0.4462\n",
      "Epoch 56/100\n",
      "143/143 [==============================] - 8s 52ms/step - loss: 0.0885 - accuracy: 0.4326 - val_loss: 0.0226 - val_accuracy: 0.4468\n",
      "Epoch 57/100\n",
      "143/143 [==============================] - 7s 52ms/step - loss: 0.0853 - accuracy: 0.4346 - val_loss: 0.0213 - val_accuracy: 0.4460\n",
      "Epoch 58/100\n",
      "143/143 [==============================] - 7s 52ms/step - loss: 0.0802 - accuracy: 0.4367 - val_loss: 0.0198 - val_accuracy: 0.4481\n",
      "Epoch 59/100\n",
      "143/143 [==============================] - 8s 53ms/step - loss: 0.0769 - accuracy: 0.4357 - val_loss: 0.0185 - val_accuracy: 0.4471\n",
      "Epoch 60/100\n",
      "143/143 [==============================] - 8s 52ms/step - loss: 0.0724 - accuracy: 0.4367 - val_loss: 0.0175 - val_accuracy: 0.4463\n",
      "Epoch 61/100\n",
      "143/143 [==============================] - 7s 52ms/step - loss: 0.0711 - accuracy: 0.4354 - val_loss: 0.0171 - val_accuracy: 0.4526\n",
      "Epoch 62/100\n",
      "143/143 [==============================] - 7s 52ms/step - loss: 0.0655 - accuracy: 0.4373 - val_loss: 0.0182 - val_accuracy: 0.4483\n",
      "Epoch 63/100\n",
      "143/143 [==============================] - 8s 53ms/step - loss: 0.0649 - accuracy: 0.4382 - val_loss: 0.0135 - val_accuracy: 0.4486\n",
      "Epoch 64/100\n",
      "143/143 [==============================] - 8s 53ms/step - loss: 0.0601 - accuracy: 0.4389 - val_loss: 0.0136 - val_accuracy: 0.4460\n",
      "Epoch 65/100\n",
      "143/143 [==============================] - 8s 53ms/step - loss: 0.0581 - accuracy: 0.4391 - val_loss: 0.0127 - val_accuracy: 0.4549\n",
      "Epoch 66/100\n",
      "143/143 [==============================] - 8s 53ms/step - loss: 0.0578 - accuracy: 0.4394 - val_loss: 0.0130 - val_accuracy: 0.4472\n",
      "Epoch 67/100\n",
      "143/143 [==============================] - 8s 53ms/step - loss: 0.0536 - accuracy: 0.4393 - val_loss: 0.0119 - val_accuracy: 0.4473\n",
      "Epoch 68/100\n",
      "143/143 [==============================] - 7s 52ms/step - loss: 0.0504 - accuracy: 0.4402 - val_loss: 0.0118 - val_accuracy: 0.4522\n",
      "Epoch 69/100\n",
      "143/143 [==============================] - 7s 52ms/step - loss: 0.0511 - accuracy: 0.4410 - val_loss: 0.0110 - val_accuracy: 0.4514\n",
      "Epoch 70/100\n",
      "143/143 [==============================] - 7s 52ms/step - loss: 0.0485 - accuracy: 0.4414 - val_loss: 0.0092 - val_accuracy: 0.4447\n",
      "Epoch 71/100\n",
      "143/143 [==============================] - 7s 52ms/step - loss: 0.0473 - accuracy: 0.4418 - val_loss: 0.0087 - val_accuracy: 0.4510\n",
      "Epoch 72/100\n",
      "143/143 [==============================] - 7s 52ms/step - loss: 0.0447 - accuracy: 0.4422 - val_loss: 0.0107 - val_accuracy: 0.4477\n",
      "Epoch 73/100\n",
      "143/143 [==============================] - 8s 52ms/step - loss: 0.0440 - accuracy: 0.4446 - val_loss: 0.0087 - val_accuracy: 0.4441\n",
      "Epoch 74/100\n",
      "143/143 [==============================] - 7s 52ms/step - loss: 0.0422 - accuracy: 0.4439 - val_loss: 0.0088 - val_accuracy: 0.4555\n",
      "Epoch 75/100\n",
      "143/143 [==============================] - 8s 53ms/step - loss: 0.0412 - accuracy: 0.4428 - val_loss: 0.0089 - val_accuracy: 0.4499\n",
      "Epoch 76/100\n",
      "143/143 [==============================] - 7s 52ms/step - loss: 0.0405 - accuracy: 0.4414 - val_loss: 0.0088 - val_accuracy: 0.4432\n"
     ]
    }
   ],
   "source": [
    "EPOCHS = 100  # 에포크 수\n",
    "from tensorflow.keras.callbacks import EarlyStopping\n",
    "\n",
    "early_stopping = EarlyStopping(\n",
    "    monitor='val_loss',        # 'val_loss'를 모니터링\n",
    "    patience=3,                # 3 에포크 동안 성능 향상이 없으면 학습 중단\n",
    "    restore_best_weights=True  # 가장 좋은 모델 가중치를 복원\n",
    ")\n",
    "\n",
    "# 모델 컴파일\n",
    "model2.compile(optimizer=optimizer, loss=loss_function, metrics=[accuracy])\n",
    "\n",
    "# 모델 학습\n",
    "history = model2.fit(\n",
    "    train_dataset,  # 훈련 데이터\n",
    "    epochs=EPOCHS,  # 에포크 수\n",
    "    validation_data=val_dataset,  # 검증 데이터\n",
    "    callbacks=[early_stopping],  # 조기 종료 콜백\n",
    "    verbose=1\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 230,
   "id": "7245aac3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "질문: 결혼하면 좋아?\n",
      "예측된 답변: 해봐요\n"
     ]
    }
   ],
   "source": [
    "# 예시로 질문 넣기\n",
    "input_question = \"결혼하면 좋아?\"\n",
    "answer = predict_answer(input_question, tokenizer, model2)\n",
    "\n",
    "# 결과 출력\n",
    "print(\"질문:\", input_question)\n",
    "print(\"예측된 답변:\", answer)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 241,
   "id": "9e842c35",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "질문: 너도 힘들지? 사람들 답변해주느라\n",
      "예측된 답변: 술 한잔으로 잊혀질 수 있다면 얼마나 좋을까요\n"
     ]
    }
   ],
   "source": [
    "# 예시로 질문 넣기\n",
    "input_question = \"너도 힘들지? 사람들 답변해주느라\"\n",
    "answer = predict_answer(input_question, tokenizer, model)\n",
    "\n",
    "# 결과 출력\n",
    "print(\"질문:\", input_question)\n",
    "print(\"예측된 답변:\", answer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 247,
   "id": "99648cf4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "질문: 내가 좋아하는 마음이 다른 사람에게 상처가 되면 어떡해\n",
      "예측된 답변: 그 사람도 설렐 거예요\n"
     ]
    }
   ],
   "source": [
    "# 예시로 질문 넣기\n",
    "input_question = \"내가 좋아하는 마음이 다른 사람에게 상처가 되면 어떡해\"\n",
    "answer = predict_answer(input_question, tokenizer, model)\n",
    "\n",
    "# 결과 출력\n",
    "print(\"질문:\", input_question)\n",
    "print(\"예측된 답변:\", answer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 231,
   "id": "c34a0c61",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "질문: 자살 생각이 들어..\n",
      "예측된 답변: 정말 그래요 하나같이 공감되곤 하죠\n"
     ]
    }
   ],
   "source": [
    "# 예시로 질문 넣기\n",
    "input_question = \"자살 생각이 들어..\"\n",
    "answer = predict_answer(input_question, tokenizer, model2)\n",
    "\n",
    "# 결과 출력\n",
    "print(\"질문:\", input_question)\n",
    "print(\"예측된 답변:\", answer)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 232,
   "id": "fa46c43c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "질문: 나랑 사귀자 내가 잘해줄게\n",
      "예측된 답변: 이제 같은 실수 안 하면 돼요\n"
     ]
    }
   ],
   "source": [
    "# 예시로 질문 넣기\n",
    "input_question = \"나랑 사귀자 내가 잘해줄게\"\n",
    "answer = predict_answer(input_question, tokenizer, model)\n",
    "\n",
    "# 결과 출력\n",
    "print(\"질문:\", input_question)\n",
    "print(\"예측된 답변:\", answer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 233,
   "id": "f97030a3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "질문: 인생이란 뭘까\n",
      "예측된 답변: 스트레스 받지 마세요\n"
     ]
    }
   ],
   "source": [
    "# 예시로 질문 넣기\n",
    "input_question = \"인생이란 뭘까\"\n",
    "answer = predict_answer(input_question, tokenizer, model)\n",
    "\n",
    "# 결과 출력\n",
    "print(\"질문:\", input_question)\n",
    "print(\"예측된 답변:\", answer)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 234,
   "id": "bfc17546",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "질문: 피곤하네\n",
      "예측된 답변: 아무래도 그렇겠죠\n"
     ]
    }
   ],
   "source": [
    "# 예시로 질문 넣기\n",
    "input_question = \"피곤하네\"\n",
    "answer = predict_answer(input_question, tokenizer, model)\n",
    "\n",
    "# 결과 출력\n",
    "print(\"질문:\", input_question)\n",
    "print(\"예측된 답변:\", answer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21a4200c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "ca6b33d5",
   "metadata": {},
   "source": [
    "### 회고\n",
    "실험을 통해 다음과 같은 사실을 알게되었습니다.\n",
    "\n",
    "- train, valid 나눠서 돌리니까 일반화 성능이 더 올라감\n",
    "\n",
    "- 데이터셋 규모에 비해 모델이 무거우면 과적합이 빠르게 진행됨"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c42166c5",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "739efe4d",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
