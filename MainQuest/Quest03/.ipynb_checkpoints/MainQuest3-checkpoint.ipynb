{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f5cfad48",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a6874ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "def text_preprocessing(text):\n",
    "    # 1. NaN 방지: 문자열 변환\n",
    "    if pd.isnull(text):\n",
    "        return \"\"\n",
    "    \n",
    "    # 2. 특수문자 제거 (한글, 영어, 숫자, 공백만 남김)\n",
    "    text = re.sub(r\"[^가-힣a-zA-Z0-9\\s]\", \"\", text)\n",
    "    \n",
    "    # 3. 양쪽 공백 제거\n",
    "    text = text.strip()\n",
    "    \n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "51004b87",
   "metadata": {},
   "outputs": [],
   "source": [
    "# sentencepiece 토크나이저 함수\n",
    "\n",
    "def sp_tokenize(s, corpus, padding='pre'):\n",
    "    tensor = []\n",
    "\n",
    "    for sen in corpus:\n",
    "#         tensor.append(s.EncodeAsIds(sen))\n",
    "        tensor.append(s.encode_as_ids(sen))\n",
    "    \n",
    "    with open(\"spm.vocab\", 'r') as f:\n",
    "        vocab = f.readlines()\n",
    "\n",
    "    word_index = {}\n",
    "    index_word = {}\n",
    "\n",
    "    for idx, line in enumerate(vocab):\n",
    "        word = line.split(\"\\t\")[0]\n",
    "\n",
    "        word_index.update({word:idx})\n",
    "        index_word.update({idx:word})\n",
    "\n",
    "    tensor = tf.keras.preprocessing.sequence.pad_sequences(tensor, padding=padding)\n",
    "\n",
    "    return tensor, word_index, index_word"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "66232c85",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "d980c519",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data = pd.read_csv('ratings_train.txt', sep='\\t') # train 불러오기\n",
    "test_data = pd.read_csv('ratings_test.txt', sep='\\t') # test 불러오기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "4528b83f",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data['document'] = train_data['document'].apply(text_preprocessing)\n",
    "test_data['document'] = test_data['document'].apply(text_preprocessing)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "95ce4438",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "150000\n",
      "50000\n"
     ]
    }
   ],
   "source": [
    "# row, column의 수가 제대로 읽혔는지 확인\n",
    "print(len(train_data['document']))      # nrows: 150000\n",
    "print(len(test_data['document']))       # nrows: 50000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "01a045e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data = train_data.drop_duplicates('document').dropna()\n",
    "test_data = test_data.drop_duplicates('document').dropna()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "45c02584",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "143475\n",
      "48437\n"
     ]
    }
   ],
   "source": [
    "# row, column의 수가 제대로 읽혔는지 확인\n",
    "print(len(train_data['document']))      # nrows: 143475\n",
    "print(len(test_data['document']))       # nrows: 48437"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "a0f7a082",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sentencepiece as spm\n",
    "import os"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "499b25b8",
   "metadata": {},
   "source": [
    "## unigram 사용"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "fe32c33b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "sentencepiece_trainer.cc(77) LOG(INFO) Starts training with : \n",
      "trainer_spec {\n",
      "  input: /aiffel/aiffel/sp_tokenizer/data/korean-english-park.train.ko.temp\n",
      "  input_format: \n",
      "  model_prefix: spm\n",
      "  model_type: UNIGRAM\n",
      "  vocab_size: 8000\n",
      "  self_test_sample_size: 0\n",
      "  character_coverage: 0.9995\n",
      "  input_sentence_size: 0\n",
      "  shuffle_input_sentence: 1\n",
      "  seed_sentencepiece_size: 1000000\n",
      "  shrinking_factor: 0.75\n",
      "  max_sentence_length: 4192\n",
      "  num_threads: 16\n",
      "  num_sub_iterations: 2\n",
      "  max_sentencepiece_length: 16\n",
      "  split_by_unicode_script: 1\n",
      "  split_by_number: 1\n",
      "  split_by_whitespace: 1\n",
      "  split_digits: 0\n",
      "  treat_whitespace_as_suffix: 0\n",
      "  allow_whitespace_only_pieces: 0\n",
      "  required_chars: \n",
      "  byte_fallback: 0\n",
      "  vocabulary_output_piece_score: 1\n",
      "  train_extremely_large_corpus: 0\n",
      "  hard_vocab_limit: 1\n",
      "  use_all_vocab: 0\n",
      "  unk_id: 0\n",
      "  bos_id: 1\n",
      "  eos_id: 2\n",
      "  pad_id: -1\n",
      "  unk_piece: <unk>\n",
      "  bos_piece: <s>\n",
      "  eos_piece: </s>\n",
      "  pad_piece: <pad>\n",
      "  unk_surface:  ⁇ \n",
      "}\n",
      "normalizer_spec {\n",
      "  name: nmt_nfkc\n",
      "  add_dummy_prefix: 1\n",
      "  remove_extra_whitespaces: 1\n",
      "  escape_whitespaces: 1\n",
      "  normalization_rule_tsv: \n",
      "}\n",
      "denormalizer_spec {}\n",
      "trainer_interface.cc(329) LOG(INFO) SentenceIterator is not specified. Using MultiFileSentenceIterator.\n",
      "trainer_interface.cc(178) LOG(INFO) Loading corpus: /aiffel/aiffel/sp_tokenizer/data/korean-english-park.train.ko.temp\n",
      "trainer_interface.cc(385) LOG(INFO) Loaded all 76908 sentences\n",
      "trainer_interface.cc(400) LOG(INFO) Adding meta_piece: <unk>\n",
      "trainer_interface.cc(400) LOG(INFO) Adding meta_piece: <s>\n",
      "trainer_interface.cc(400) LOG(INFO) Adding meta_piece: </s>\n",
      "trainer_interface.cc(405) LOG(INFO) Normalizing sentences...\n",
      "trainer_interface.cc(466) LOG(INFO) all chars count=4996369\n",
      "trainer_interface.cc(477) LOG(INFO) Done: 99.95% characters are covered.\n",
      "trainer_interface.cc(487) LOG(INFO) Alphabet size=1317\n",
      "trainer_interface.cc(488) LOG(INFO) Final character coverage=0.9995\n",
      "trainer_interface.cc(520) LOG(INFO) Done! preprocessed 76908 sentences.\n",
      "unigram_model_trainer.cc(139) LOG(INFO) Making suffix array...\n",
      "unigram_model_trainer.cc(143) LOG(INFO) Extracting frequent sub strings...\n",
      "unigram_model_trainer.cc(194) LOG(INFO) Initialized 174340 seed sentencepieces\n",
      "trainer_interface.cc(526) LOG(INFO) Tokenizing input sentences with whitespace: 76908\n",
      "trainer_interface.cc(537) LOG(INFO) Done! 237965\n",
      "unigram_model_trainer.cc(489) LOG(INFO) Using 237965 sentences for EM training\n",
      "unigram_model_trainer.cc(505) LOG(INFO) EM sub_iter=0 size=92555 obj=14.853 num_tokens=523272 num_tokens/piece=5.65363\n",
      "unigram_model_trainer.cc(505) LOG(INFO) EM sub_iter=1 size=82083 obj=13.516 num_tokens=525776 num_tokens/piece=6.40542\n",
      "unigram_model_trainer.cc(505) LOG(INFO) EM sub_iter=0 size=61555 obj=13.5533 num_tokens=546907 num_tokens/piece=8.88485\n",
      "unigram_model_trainer.cc(505) LOG(INFO) EM sub_iter=1 size=61506 obj=13.5101 num_tokens=547350 num_tokens/piece=8.89913\n",
      "unigram_model_trainer.cc(505) LOG(INFO) EM sub_iter=0 size=46126 obj=13.6926 num_tokens=575369 num_tokens/piece=12.4739\n",
      "unigram_model_trainer.cc(505) LOG(INFO) EM sub_iter=1 size=46126 obj=13.6493 num_tokens=575466 num_tokens/piece=12.476\n",
      "unigram_model_trainer.cc(505) LOG(INFO) EM sub_iter=0 size=34594 obj=13.8894 num_tokens=606014 num_tokens/piece=17.5179\n",
      "unigram_model_trainer.cc(505) LOG(INFO) EM sub_iter=1 size=34594 obj=13.8387 num_tokens=606012 num_tokens/piece=17.5178\n",
      "unigram_model_trainer.cc(505) LOG(INFO) EM sub_iter=0 size=25945 obj=14.1301 num_tokens=637532 num_tokens/piece=24.5724\n",
      "unigram_model_trainer.cc(505) LOG(INFO) EM sub_iter=1 size=25945 obj=14.0747 num_tokens=637568 num_tokens/piece=24.5738\n",
      "unigram_model_trainer.cc(505) LOG(INFO) EM sub_iter=0 size=19458 obj=14.4091 num_tokens=670960 num_tokens/piece=34.4825\n",
      "unigram_model_trainer.cc(505) LOG(INFO) EM sub_iter=1 size=19458 obj=14.3468 num_tokens=670999 num_tokens/piece=34.4845\n",
      "unigram_model_trainer.cc(505) LOG(INFO) EM sub_iter=0 size=14593 obj=14.7196 num_tokens=705636 num_tokens/piece=48.3544\n",
      "unigram_model_trainer.cc(505) LOG(INFO) EM sub_iter=1 size=14593 obj=14.648 num_tokens=705645 num_tokens/piece=48.355\n",
      "unigram_model_trainer.cc(505) LOG(INFO) EM sub_iter=0 size=10944 obj=15.0875 num_tokens=741620 num_tokens/piece=67.765\n",
      "unigram_model_trainer.cc(505) LOG(INFO) EM sub_iter=1 size=10944 obj=15.007 num_tokens=741624 num_tokens/piece=67.7654\n",
      "unigram_model_trainer.cc(505) LOG(INFO) EM sub_iter=0 size=8800 obj=15.3757 num_tokens=769363 num_tokens/piece=87.4276\n",
      "unigram_model_trainer.cc(505) LOG(INFO) EM sub_iter=1 size=8800 obj=15.307 num_tokens=769367 num_tokens/piece=87.4281\n",
      "trainer_interface.cc(615) LOG(INFO) Saving model: spm.model\n",
      "trainer_interface.cc(626) LOG(INFO) Saving vocabs: spm.vocab\n"
     ]
    }
   ],
   "source": [
    "# sentencepiece 학습\n",
    "import sentencepiece as spm\n",
    "vocab_size = 8000\n",
    "spm.SentencePieceTrainer.Train(\n",
    "#     input='spm_input.txt',\n",
    "    input = os.getenv('HOME')+'/aiffel/sp_tokenizer/data/korean-english-park.train.ko.temp',\n",
    "    model_prefix='spm',\n",
    "    vocab_size=vocab_size,\n",
    "    model_type='unigram',\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "58b093f8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sp = spm.SentencePieceProcessor()\n",
    "sp.load(\"spm.model\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "b1e03a99",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_tokens = sp_tokenize(sp, list(train_data['document']), padding='pre')\n",
    "test_tokens = sp_tokenize(sp, list(test_data['document']), padding='pre')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "3e7e3d2a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Embedding, LSTM, Dense\n",
    "from tensorflow.keras.callbacks import EarlyStopping, ModelCheckpoint, ReduceLROnPlateau"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "2a738237",
   "metadata": {},
   "outputs": [],
   "source": [
    "early_stop = EarlyStopping(monitor='val_loss', patience=3, restore_best_weights=True, verbose=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "0233edbc",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Sequential([\n",
    "    Embedding(input_dim=vocab_size, output_dim=256),\n",
    "    LSTM(128),\n",
    "    Dense(256),\n",
    "    Dense(1, activation='sigmoid')\n",
    "])\n",
    "\n",
    "model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "c4b9bd45",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "2242/2242 [==============================] - 28s 11ms/step - loss: 0.4123 - accuracy: 0.8095 - val_loss: 0.3693 - val_accuracy: 0.8345\n",
      "Epoch 2/10\n",
      "2242/2242 [==============================] - 24s 11ms/step - loss: 0.3318 - accuracy: 0.8547 - val_loss: 0.3522 - val_accuracy: 0.8448\n",
      "Epoch 3/10\n",
      "2242/2242 [==============================] - 24s 11ms/step - loss: 0.2903 - accuracy: 0.8761 - val_loss: 0.3438 - val_accuracy: 0.8471\n",
      "Epoch 4/10\n",
      "2242/2242 [==============================] - 24s 11ms/step - loss: 0.2517 - accuracy: 0.8947 - val_loss: 0.3608 - val_accuracy: 0.8491\n",
      "Epoch 5/10\n",
      "2242/2242 [==============================] - 24s 11ms/step - loss: 0.2150 - accuracy: 0.9129 - val_loss: 0.3810 - val_accuracy: 0.8476\n",
      "Epoch 6/10\n",
      "2242/2242 [==============================] - 24s 11ms/step - loss: 0.1776 - accuracy: 0.9301 - val_loss: 0.4101 - val_accuracy: 0.8471\n"
     ]
    }
   ],
   "source": [
    "history = model.fit(\n",
    "    train_tokens[0], train_data['label'].values,\n",
    "    validation_data=(test_tokens[0], test_data['label'].values),\n",
    "    epochs=10,\n",
    "    batch_size=64,\n",
    "    callbacks=[early_stop]\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "10b86605",
   "metadata": {},
   "source": [
    "val_acc = 84.91"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bcd9449f",
   "metadata": {},
   "source": [
    "## bpe 사용"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "8df3127a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "sentencepiece_trainer.cc(77) LOG(INFO) Starts training with : \n",
      "trainer_spec {\n",
      "  input: /aiffel/aiffel/sp_tokenizer/data/korean-english-park.train.ko.temp\n",
      "  input_format: \n",
      "  model_prefix: spm\n",
      "  model_type: BPE\n",
      "  vocab_size: 8000\n",
      "  self_test_sample_size: 0\n",
      "  character_coverage: 0.9995\n",
      "  input_sentence_size: 0\n",
      "  shuffle_input_sentence: 1\n",
      "  seed_sentencepiece_size: 1000000\n",
      "  shrinking_factor: 0.75\n",
      "  max_sentence_length: 4192\n",
      "  num_threads: 16\n",
      "  num_sub_iterations: 2\n",
      "  max_sentencepiece_length: 16\n",
      "  split_by_unicode_script: 1\n",
      "  split_by_number: 1\n",
      "  split_by_whitespace: 1\n",
      "  split_digits: 0\n",
      "  treat_whitespace_as_suffix: 0\n",
      "  allow_whitespace_only_pieces: 0\n",
      "  required_chars: \n",
      "  byte_fallback: 0\n",
      "  vocabulary_output_piece_score: 1\n",
      "  train_extremely_large_corpus: 0\n",
      "  hard_vocab_limit: 1\n",
      "  use_all_vocab: 0\n",
      "  unk_id: 0\n",
      "  bos_id: 1\n",
      "  eos_id: 2\n",
      "  pad_id: -1\n",
      "  unk_piece: <unk>\n",
      "  bos_piece: <s>\n",
      "  eos_piece: </s>\n",
      "  pad_piece: <pad>\n",
      "  unk_surface:  ⁇ \n",
      "}\n",
      "normalizer_spec {\n",
      "  name: nmt_nfkc\n",
      "  add_dummy_prefix: 1\n",
      "  remove_extra_whitespaces: 1\n",
      "  escape_whitespaces: 1\n",
      "  normalization_rule_tsv: \n",
      "}\n",
      "denormalizer_spec {}\n",
      "trainer_interface.cc(329) LOG(INFO) SentenceIterator is not specified. Using MultiFileSentenceIterator.\n",
      "trainer_interface.cc(178) LOG(INFO) Loading corpus: /aiffel/aiffel/sp_tokenizer/data/korean-english-park.train.ko.temp\n",
      "trainer_interface.cc(385) LOG(INFO) Loaded all 76908 sentences\n",
      "trainer_interface.cc(400) LOG(INFO) Adding meta_piece: <unk>\n",
      "trainer_interface.cc(400) LOG(INFO) Adding meta_piece: <s>\n",
      "trainer_interface.cc(400) LOG(INFO) Adding meta_piece: </s>\n",
      "trainer_interface.cc(405) LOG(INFO) Normalizing sentences...\n",
      "trainer_interface.cc(466) LOG(INFO) all chars count=4996369\n",
      "trainer_interface.cc(477) LOG(INFO) Done: 99.95% characters are covered.\n",
      "trainer_interface.cc(487) LOG(INFO) Alphabet size=1317\n",
      "trainer_interface.cc(488) LOG(INFO) Final character coverage=0.9995\n",
      "trainer_interface.cc(520) LOG(INFO) Done! preprocessed 76908 sentences.\n",
      "trainer_interface.cc(526) LOG(INFO) Tokenizing input sentences with whitespace: 76908\n",
      "trainer_interface.cc(537) LOG(INFO) Done! 237965\n",
      "bpe_model_trainer.cc(167) LOG(INFO) Updating active symbols. max_freq=37043 min_freq=188\n",
      "bpe_model_trainer.cc(258) LOG(INFO) Added: freq=10526 size=20 all=64102 active=6239 piece=▁가\n",
      "bpe_model_trainer.cc(258) LOG(INFO) Added: freq=7586 size=40 all=66979 active=9116 piece=▁일\n",
      "bpe_model_trainer.cc(258) LOG(INFO) Added: freq=5712 size=60 all=69013 active=11150 piece=▁하\n",
      "bpe_model_trainer.cc(258) LOG(INFO) Added: freq=4404 size=80 all=71006 active=13143 piece=▁해\n",
      "bpe_model_trainer.cc(258) LOG(INFO) Added: freq=3904 size=100 all=72666 active=14803 piece=▁결\n",
      "bpe_model_trainer.cc(167) LOG(INFO) Updating active symbols. max_freq=3872 min_freq=147\n",
      "bpe_model_trainer.cc(258) LOG(INFO) Added: freq=3422 size=120 all=74308 active=5232 piece=▁반\n",
      "bpe_model_trainer.cc(258) LOG(INFO) Added: freq=3011 size=140 all=76157 active=7081 piece=들을\n",
      "bpe_model_trainer.cc(258) LOG(INFO) Added: freq=2743 size=160 all=77986 active=8910 piece=▁에\n",
      "bpe_model_trainer.cc(258) LOG(INFO) Added: freq=2451 size=180 all=79566 active=10490 piece=한다\n",
      "bpe_model_trainer.cc(258) LOG(INFO) Added: freq=2222 size=200 all=80983 active=11907 piece=에는\n",
      "bpe_model_trainer.cc(167) LOG(INFO) Updating active symbols. max_freq=2209 min_freq=126\n",
      "bpe_model_trainer.cc(258) LOG(INFO) Added: freq=2027 size=220 all=82597 active=5345 piece=▁지역\n",
      "bpe_model_trainer.cc(258) LOG(INFO) Added: freq=1878 size=240 all=83697 active=6445 piece=▁한편\n",
      "bpe_model_trainer.cc(258) LOG(INFO) Added: freq=1732 size=260 all=85129 active=7877 piece=▁보도\n",
      "bpe_model_trainer.cc(258) LOG(INFO) Added: freq=1623 size=280 all=86544 active=9292 piece=▁관련\n",
      "bpe_model_trainer.cc(258) LOG(INFO) Added: freq=1557 size=300 all=87836 active=10584 piece=▁대통령은\n",
      "bpe_model_trainer.cc(167) LOG(INFO) Updating active symbols. max_freq=1556 min_freq=113\n",
      "bpe_model_trainer.cc(258) LOG(INFO) Added: freq=1510 size=320 all=8"
     ]
    }
   ],
   "source": [
    "# sentencepiece 학습\n",
    "import sentencepiece as spm\n",
    "vocab_size = 8000\n",
    "spm.SentencePieceTrainer.Train(\n",
    "#     input='spm_input.txt',\n",
    "    input = os.getenv('HOME')+'/aiffel/sp_tokenizer/data/korean-english-park.train.ko.temp',\n",
    "    model_prefix='spm',\n",
    "    vocab_size=vocab_size,\n",
    "    model_type='bpe',\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "df5fa07a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sp = spm.SentencePieceProcessor()\n",
    "sp.load(\"spm.model\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "1aeb1d14",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_tokens = sp_tokenize(sp, list(train_data['document']), padding='pre')\n",
    "test_tokens = sp_tokenize(sp, list(test_data['document']), padding='pre')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "8f812e11",
   "metadata": {},
   "outputs": [],
   "source": [
    "del model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "7d2b90c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Sequential([\n",
    "    Embedding(input_dim=vocab_size, output_dim=256),\n",
    "    LSTM(128),\n",
    "    Dense(256),\n",
    "    Dense(1, activation='sigmoid')\n",
    "])\n",
    "\n",
    "model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "154f6cf5",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "2242/2242 [==============================] - 25s 10ms/step - loss: 0.4128 - accuracy: 0.8096 - val_loss: 0.3682 - val_accuracy: 0.8343\n",
      "Epoch 2/10\n",
      "2242/2242 [==============================] - 23s 10ms/step - loss: 0.3327 - accuracy: 0.8535 - val_loss: 0.3439 - val_accuracy: 0.8466\n",
      "Epoch 3/10\n",
      "2242/2242 [==============================] - 23s 10ms/step - loss: 0.2884 - accuracy: 0.8774 - val_loss: 0.3419 - val_accuracy: 0.8505\n",
      "Epoch 4/10\n",
      "2242/2242 [==============================] - 23s 10ms/step - loss: 0.2493 - accuracy: 0.8968 - val_loss: 0.3634 - val_accuracy: 0.8496\n",
      "Epoch 5/10\n",
      "2242/2242 [==============================] - 23s 10ms/step - loss: 0.2099 - accuracy: 0.9155 - val_loss: 0.3841 - val_accuracy: 0.8460\n",
      "Epoch 6/10\n",
      "2242/2242 [==============================] - 23s 10ms/step - loss: 0.1713 - accuracy: 0.9328 - val_loss: 0.4299 - val_accuracy: 0.8445\n"
     ]
    }
   ],
   "source": [
    "history = model.fit(\n",
    "    train_tokens[0], train_data['label'].values,\n",
    "    validation_data=(test_tokens[0], test_data['label'].values),\n",
    "    epochs=10,\n",
    "    batch_size=64,\n",
    "    callbacks=[early_stop]\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dd896a6a",
   "metadata": {},
   "source": [
    "val_acc = 85.05"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f8bf3c5",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c5322a23",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e119fb1c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "926e5307",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "4196283f",
   "metadata": {},
   "source": [
    "## konlpy 사용"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e6699ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "from konlpy.tag import Mecab\n",
    "from collections import Counter\n",
    "\n",
    "mecab = Mecab()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e7fd1f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize(doc):\n",
    "    # norm, stem은 optional\n",
    "    return ['/'.join(t) for t in mecab.pos(doc)]\n",
    "\n",
    "train_docs = [tokenize(row) for row in train_data['document']]\n",
    "test_docs = [tokenize(row) for row in test_data['document']]\n",
    "tokens = [t for d in train_docs for t in d]\n",
    "all_tokens = [token for tokens in train_docs for token in tokens]\n",
    "counter = Counter(all_tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c4eba456",
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import Counter\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "import tensorflow as tf\n",
    "\n",
    "vocab_size = 8000\n",
    "most_common_tokens = [token for token, _ in counter.most_common(vocab_size)]\n",
    "vocab_set = set(most_common_tokens)\n",
    "\n",
    "print(\"✅ vocab 생성 완료:\", len(vocab_set))\n",
    "\n",
    "# 3. <UNK> 처리 함수\n",
    "def filter_tokens(tokens, vocab_set):\n",
    "    return [token if token in vocab_set else \"<UNK>\" for token in tokens]\n",
    "\n",
    "# 4. train/test 문서에 vocab 적용\n",
    "filtered_train_docs = [filter_tokens(doc, vocab_set) for doc in train_docs]\n",
    "filtered_test_docs = [filter_tokens(doc, vocab_set) for doc in test_docs]\n",
    "\n",
    "# 5. Tokenizer 정의 및 word_index 설정\n",
    "tokenizer = Tokenizer(num_words=vocab_size + 2, oov_token=\"<UNK>\")\n",
    "tokenizer.word_index = {word: idx + 1 for idx, word in enumerate(most_common_tokens)}\n",
    "tokenizer.word_index[tokenizer.oov_token] = vocab_size + 1\n",
    "\n",
    "# 6. 텍스트를 시퀀스로 변환\n",
    "train_texts = [\" \".join(doc) for doc in filtered_train_docs]\n",
    "test_texts = [\" \".join(doc) for doc in filtered_test_docs]\n",
    "\n",
    "train_sequences = tokenizer.texts_to_sequences(train_texts)\n",
    "test_sequences = tokenizer.texts_to_sequences(test_texts)\n",
    "\n",
    "# 7. 시퀀스 padding\n",
    "max_len = 50\n",
    "train_padded = pad_sequences(train_sequences, maxlen=max_len, padding='post')\n",
    "test_padded = pad_sequences(test_sequences, maxlen=max_len, padding='post')\n",
    "\n",
    "# 8. 레이블 분리\n",
    "train_labels = train_data['label'].tolist()\n",
    "test_labels = test_data['label'].tolist()\n",
    "\n",
    "# 9. TensorFlow Dataset 생성\n",
    "batch_size = 64\n",
    "\n",
    "train_dataset = tf.data.Dataset.from_tensor_slices((train_padded, train_labels))\n",
    "train_dataset = train_dataset.shuffle(len(train_padded)).batch(batch_size).prefetch(tf.data.AUTOTUNE)\n",
    "\n",
    "test_dataset = tf.data.Dataset.from_tensor_slices((test_padded, test_labels))\n",
    "test_dataset = test_dataset.batch(batch_size).prefetch(tf.data.AUTOTUNE)\n",
    "\n",
    "# 10. 확인 출력\n",
    "print(\"✅ 데이터셋 준비 완료\")\n",
    "print(\"Train dataset shape:\", train_padded.shape)\n",
    "print(\"Test dataset shape:\", test_padded.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf1acb10",
   "metadata": {},
   "outputs": [],
   "source": [
    "del model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "34f4d85e",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Sequential([\n",
    "    Embedding(input_dim=vocab_size, output_dim=256),\n",
    "    LSTM(128),\n",
    "    Dense(256),\n",
    "    Dense(1, activation='sigmoid')\n",
    "])\n",
    "\n",
    "model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d5fe56f",
   "metadata": {},
   "outputs": [],
   "source": [
    "history = model.fit(\n",
    "    train_dataset,\n",
    "    validation_data=test_dataset,\n",
    "    epochs=10,\n",
    "    callbacks=[early_stop]\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dcb65b04",
   "metadata": {},
   "source": [
    "val_acc = "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da3b618d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b9cc837a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "539223e9",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba8e2800",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
