{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b578323f",
   "metadata": {},
   "source": [
    "data 출처 : https://github.com/songys/Chatbot_data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fc1793d2",
   "metadata": {},
   "source": [
    "### GPT1 모델 구조 정의\n",
    "- transformer에서 변경된 모델 구조\n",
    "\n",
    "1. 인코더 삭제\n",
    "\n",
    "2. 인코더 디코더 어텐션 삭제\n",
    "\n",
    "3. 포지셔널 인코딩을 삭제하고 포지셔널 임베딩으로 대체\n",
    "\n",
    "4. 활성화 함수 gelu 사용"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 333,
   "id": "8e932146",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import tensorflow_datasets as tfds\n",
    "import os\n",
    "import re\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 334,
   "id": "9c8627b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 스케일드 닷 프로덕트 어텐션 함수\n",
    "def scaled_dot_product_attention(query, key, value, mask):\n",
    "  # 어텐션 가중치는 Q와 K의 닷 프로덕트\n",
    "    matmul_qk = tf.matmul(query, key, transpose_b=True)\n",
    "\n",
    "    # 가중치를 정규화\n",
    "    depth = tf.cast(tf.shape(key)[-1], tf.float32)\n",
    "    logits = matmul_qk / tf.math.sqrt(depth)\n",
    "\n",
    "    # 패딩에 마스크 추가\n",
    "    if mask is not None:\n",
    "        logits += (mask * -1e9)\n",
    "\n",
    "    # softmax적용\n",
    "    attention_weights = tf.nn.softmax(logits, axis=-1)\n",
    "\n",
    "    # 최종 어텐션은 가중치와 V의 닷 프로덕트\n",
    "    output = tf.matmul(attention_weights, value)\n",
    "    return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 335,
   "id": "07b300d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 멀티헤드어텐션\n",
    "class MultiHeadAttention(tf.keras.layers.Layer):\n",
    "    def __init__(self, d_model, num_heads, name=\"multi_head_attention\"):\n",
    "        super(MultiHeadAttention, self).__init__(name=name)\n",
    "        self.num_heads = num_heads\n",
    "        self.d_model = d_model\n",
    "\n",
    "        assert d_model % self.num_heads == 0\n",
    "\n",
    "        self.depth = d_model // self.num_heads\n",
    "\n",
    "        self.query_dense = tf.keras.layers.Dense(units=d_model)\n",
    "        self.key_dense = tf.keras.layers.Dense(units=d_model)\n",
    "        self.value_dense = tf.keras.layers.Dense(units=d_model)\n",
    "\n",
    "        self.dense = tf.keras.layers.Dense(units=d_model)\n",
    "\n",
    "    def split_heads(self, inputs, batch_size):\n",
    "        inputs = tf.reshape(\n",
    "            inputs, shape=(batch_size, -1, self.num_heads, self.depth))\n",
    "        return tf.transpose(inputs, perm=[0, 2, 1, 3])\n",
    "\n",
    "    def call(self, inputs):\n",
    "        query, key, value, mask = inputs['query'], inputs['key'], inputs[\n",
    "            'value'], inputs['mask']\n",
    "        batch_size = tf.shape(query)[0]\n",
    "\n",
    "        # Q, K, V에 각각 Dense를 적용합니다\n",
    "        query = self.query_dense(query)\n",
    "        key = self.key_dense(key)\n",
    "        value = self.value_dense(value)\n",
    "\n",
    "        # 병렬 연산을 위한 머리를 여러 개 만듭니다\n",
    "        query = self.split_heads(query, batch_size)\n",
    "        key = self.split_heads(key, batch_size)\n",
    "        value = self.split_heads(value, batch_size)\n",
    "\n",
    "        # 스케일드 닷 프로덕트 어텐션 함수 사용\n",
    "        scaled_attention = scaled_dot_product_attention(query, key, value, mask)\n",
    "\n",
    "        scaled_attention = tf.transpose(scaled_attention, perm=[0, 2, 1, 3])\n",
    "\n",
    "        # 어텐션 연산 후에 각 결과를 다시 연결(concatenate)합니다\n",
    "        concat_attention = tf.reshape(scaled_attention,\n",
    "                                      (batch_size, -1, self.d_model))\n",
    "\n",
    "        # 최종 결과에도 Dense를 한 번 더 적용합니다\n",
    "        outputs = self.dense(concat_attention)\n",
    "\n",
    "        return outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 336,
   "id": "abd76e85",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_padding_mask(x):\n",
    "    mask = tf.cast(tf.math.equal(x, 0), tf.float32)\n",
    "    # (batch_size, 1, 1, sequence length)\n",
    "    return mask[:, tf.newaxis, tf.newaxis, :]\n",
    "\n",
    "def create_look_ahead_mask(x):\n",
    "    seq_len = tf.shape(x)[1]\n",
    "    look_ahead_mask = 1 - tf.linalg.band_part(tf.ones((seq_len, seq_len)), -1, 0)\n",
    "    padding_mask = create_padding_mask(x)\n",
    "    return tf.maximum(look_ahead_mask, padding_mask)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 337,
   "id": "a15a4be6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 디코더 레이어 정의\n",
    "def decoder_layer(units, d_model, num_heads, dropout, name = 'decoder_layer'):\n",
    "    \n",
    "    inputs = tf.keras.Input(shape=(None, d_model), name=\"inputs\")\n",
    "    look_ahead_mask = tf.keras.Input(shape=(1, None, None), name=\"look_ahead_mask\")\n",
    "    \n",
    "    attention1 = MultiHeadAttention(\n",
    "      d_model, num_heads, name=\"attention_1\")(inputs={\n",
    "          'query': inputs,\n",
    "          'key': inputs,\n",
    "          'value': inputs,\n",
    "          'mask': look_ahead_mask\n",
    "      })\n",
    "    attention1 = tf.keras.layers.Dropout(rate=dropout)(attention1)\n",
    "    attention1 = tf.keras.layers.LayerNormalization(\n",
    "      epsilon=1e-6)(attention1 + inputs) # Residual 적용\n",
    "    \n",
    "    outputs = tf.keras.layers.Dense(units=units, activation='gelu')(attention1)\n",
    "    outputs = tf.keras.layers.Dense(units=d_model)(outputs)\n",
    "    \n",
    "    outputs = tf.keras.layers.Dropout(rate=dropout)(outputs)\n",
    "    outputs = tf.keras.layers.LayerNormalization(\n",
    "      epsilon=1e-6)(outputs + attention1) # Residual 적용\n",
    "\n",
    "    return tf.keras.Model(\n",
    "      inputs=[inputs, look_ahead_mask],\n",
    "      outputs=outputs,\n",
    "      name=name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 338,
   "id": "34f9d598",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 포지셔널 임베딩하고 디코더를 여러개 쌓는 함수\n",
    "def decoder_stack(vocab_size,\n",
    "            num_layers,\n",
    "            units,\n",
    "            d_model,\n",
    "            num_heads,\n",
    "            max_position_encoding,\n",
    "            dropout,\n",
    "            name='decoder'):\n",
    "\n",
    "    inputs = tf.keras.Input(shape=(None,), name='inputs')\n",
    "    look_ahead_mask = tf.keras.Input(\n",
    "      shape=(1, None, None), name='look_ahead_mask')\n",
    "\n",
    "    \n",
    "    seq_len = tf.shape(inputs)[1] # 현재 시퀀스의 길이\n",
    "    \n",
    "    # 토큰 임베딩\n",
    "    token_embedding_layer = tf.keras.layers.Embedding(vocab_size, d_model, name='token_embedding')\n",
    "    embeddings = token_embedding_layer(inputs)\n",
    "    \n",
    "    # 포지셔널 임베딩\n",
    "    pos_embedding_layer = tf.keras.layers.Embedding(max_position_encoding,\n",
    "                                                    d_model, name='positional_embedding')\n",
    "    positions = tf.range(start=0, limit=seq_len, delta=1)\n",
    "    positional_embeddings = pos_embedding_layer(positions)\n",
    "    \n",
    "    # 토큰 임베딩과 포지셔널 임베딩 합쳐줌\n",
    "    embeddings += positional_embeddings \n",
    "    \n",
    "    outputs = tf.keras.layers.Dropout(rate=dropout)(embeddings)\n",
    "\n",
    "    for i in range(num_layers):\n",
    "        outputs = decoder_layer(\n",
    "            units=units,\n",
    "            d_model=d_model,\n",
    "            num_heads=num_heads,\n",
    "            dropout=dropout,\n",
    "            name='decoder_layer_{}'.format(i),\n",
    "        )(inputs=[outputs, look_ahead_mask])\n",
    "\n",
    "    return tf.keras.Model(\n",
    "      inputs=[inputs, look_ahead_mask],\n",
    "      outputs=outputs,\n",
    "      name=name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 339,
   "id": "926c8290",
   "metadata": {},
   "outputs": [],
   "source": [
    "# gpt의 디코더 모델 정의\n",
    "# mask 작업도 해서 값을 디코더에 넣음\n",
    "def gpt_decoder(vocab_size,\n",
    "                num_layers,\n",
    "                units,\n",
    "                d_model,\n",
    "                num_heads,\n",
    "                dropout,\n",
    "                max_position_encoding,\n",
    "                name=\"gpt_decoder\"):\n",
    "    inputs = tf.keras.Input(shape=(None,), name=\"inputs\")\n",
    "\n",
    "    look_ahead_mask = tf.keras.layers.Lambda(\n",
    "    create_look_ahead_mask,\n",
    "    output_shape=(1, None, None),\n",
    "    name='look_ahead_mask')(inputs)\n",
    "\n",
    "    dec_outputs = decoder_stack(\n",
    "    vocab_size=vocab_size,\n",
    "    num_layers=num_layers,\n",
    "    units=units,\n",
    "    d_model=d_model,\n",
    "    num_heads=num_heads,\n",
    "    max_position_encoding=max_position_encoding,\n",
    "    dropout=dropout,)(inputs=[inputs, look_ahead_mask])\n",
    "    \n",
    "    outputs = tf.keras.layers.Dense(units=vocab_size, name=\"outputs\")(dec_outputs)\n",
    "\n",
    "    return tf.keras.Model(inputs=inputs, outputs=outputs, name=name)\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "02184b11",
   "metadata": {},
   "source": [
    "### 데이터 전처리\n",
    "- 변경된 데이터셋 구조\n",
    "1. start, q, a, end_token으로 전체 문장 시작과 q 시작, a시작, 문장 끝을 알리는 토큰을 추가하고\n",
    "하나의 시퀀스로 만듬"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 340,
   "id": "26d9653c",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_row = pd.read_csv('ChatbotData.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 341,
   "id": "b6126707",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = data_row.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 342,
   "id": "7c6aaeeb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Q</th>\n",
       "      <th>A</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>12시 땡!</td>\n",
       "      <td>하루가 또 가네요.</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1지망 학교 떨어졌어</td>\n",
       "      <td>위로해 드립니다.</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3박4일 놀러가고 싶다</td>\n",
       "      <td>여행은 언제나 좋죠.</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3박4일 정도 놀러가고 싶다</td>\n",
       "      <td>여행은 언제나 좋죠.</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>PPL 심하네</td>\n",
       "      <td>눈살이 찌푸려지죠.</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11818</th>\n",
       "      <td>훔쳐보는 것도 눈치 보임.</td>\n",
       "      <td>티가 나니까 눈치가 보이는 거죠!</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11819</th>\n",
       "      <td>훔쳐보는 것도 눈치 보임.</td>\n",
       "      <td>훔쳐보는 거 티나나봐요.</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11820</th>\n",
       "      <td>흑기사 해주는 짝남.</td>\n",
       "      <td>설렜겠어요.</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11821</th>\n",
       "      <td>힘든 연애 좋은 연애라는게 무슨 차이일까?</td>\n",
       "      <td>잘 헤어질 수 있는 사이 여부인 거 같아요.</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11822</th>\n",
       "      <td>힘들어서 결혼할까봐</td>\n",
       "      <td>도피성 결혼은 하지 않길 바라요.</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>11823 rows × 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                             Q                         A  label\n",
       "0                       12시 땡!                하루가 또 가네요.      0\n",
       "1                  1지망 학교 떨어졌어                 위로해 드립니다.      0\n",
       "2                 3박4일 놀러가고 싶다               여행은 언제나 좋죠.      0\n",
       "3              3박4일 정도 놀러가고 싶다               여행은 언제나 좋죠.      0\n",
       "4                      PPL 심하네                눈살이 찌푸려지죠.      0\n",
       "...                        ...                       ...    ...\n",
       "11818           훔쳐보는 것도 눈치 보임.        티가 나니까 눈치가 보이는 거죠!      2\n",
       "11819           훔쳐보는 것도 눈치 보임.             훔쳐보는 거 티나나봐요.      2\n",
       "11820              흑기사 해주는 짝남.                    설렜겠어요.      2\n",
       "11821  힘든 연애 좋은 연애라는게 무슨 차이일까?  잘 헤어질 수 있는 사이 여부인 거 같아요.      2\n",
       "11822               힘들어서 결혼할까봐        도피성 결혼은 하지 않길 바라요.      2\n",
       "\n",
       "[11823 rows x 3 columns]"
      ]
     },
     "execution_count": 342,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 343,
   "id": "a49747e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "data.dropna(inplace=True) # 결측치 제거\n",
    "data.drop_duplicates(inplace=True) # 중복 제거"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 344,
   "id": "3d604fa9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "def clean_text(text):\n",
    "    text = re.sub(r'[^\\w\\s가-힣]', '', text)  # 특수문자 제거 (영어 포함 가능)\n",
    "    text = re.sub(r'\\s+', ' ', text).strip()  # 중복 공백 제거\n",
    "    return text\n",
    "\n",
    "data['Q'] = data['Q'].apply(clean_text)\n",
    "data['A'] = data['A'].apply(clean_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 345,
   "id": "ea6d7ae0",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Q</th>\n",
       "      <th>A</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>12시 땡</td>\n",
       "      <td>하루가 또 가네요</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1지망 학교 떨어졌어</td>\n",
       "      <td>위로해 드립니다</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3박4일 놀러가고 싶다</td>\n",
       "      <td>여행은 언제나 좋죠</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3박4일 정도 놀러가고 싶다</td>\n",
       "      <td>여행은 언제나 좋죠</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>PPL 심하네</td>\n",
       "      <td>눈살이 찌푸려지죠</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11818</th>\n",
       "      <td>훔쳐보는 것도 눈치 보임</td>\n",
       "      <td>티가 나니까 눈치가 보이는 거죠</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11819</th>\n",
       "      <td>훔쳐보는 것도 눈치 보임</td>\n",
       "      <td>훔쳐보는 거 티나나봐요</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11820</th>\n",
       "      <td>흑기사 해주는 짝남</td>\n",
       "      <td>설렜겠어요</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11821</th>\n",
       "      <td>힘든 연애 좋은 연애라는게 무슨 차이일까</td>\n",
       "      <td>잘 헤어질 수 있는 사이 여부인 거 같아요</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11822</th>\n",
       "      <td>힘들어서 결혼할까봐</td>\n",
       "      <td>도피성 결혼은 하지 않길 바라요</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>11823 rows × 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                            Q                        A  label\n",
       "0                       12시 땡                하루가 또 가네요      0\n",
       "1                 1지망 학교 떨어졌어                 위로해 드립니다      0\n",
       "2                3박4일 놀러가고 싶다               여행은 언제나 좋죠      0\n",
       "3             3박4일 정도 놀러가고 싶다               여행은 언제나 좋죠      0\n",
       "4                     PPL 심하네                눈살이 찌푸려지죠      0\n",
       "...                       ...                      ...    ...\n",
       "11818           훔쳐보는 것도 눈치 보임        티가 나니까 눈치가 보이는 거죠      2\n",
       "11819           훔쳐보는 것도 눈치 보임             훔쳐보는 거 티나나봐요      2\n",
       "11820              흑기사 해주는 짝남                    설렜겠어요      2\n",
       "11821  힘든 연애 좋은 연애라는게 무슨 차이일까  잘 헤어질 수 있는 사이 여부인 거 같아요      2\n",
       "11822              힘들어서 결혼할까봐        도피성 결혼은 하지 않길 바라요      2\n",
       "\n",
       "[11823 rows x 3 columns]"
      ]
     },
     "execution_count": 345,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 346,
   "id": "34613864",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vocabulary size: 10125\n"
     ]
    }
   ],
   "source": [
    "questions = data['Q'].tolist()\n",
    "answers = data['A'].tolist()\n",
    "\n",
    "tokenizer = tfds.deprecated.text.SubwordTextEncoder.build_from_corpus(\n",
    "    questions + answers,\n",
    "    target_vocab_size = len(data)\n",
    ")\n",
    "\n",
    "# vocab 확인\n",
    "print(\"Vocabulary size:\", tokenizer.vocab_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 347,
   "id": "96573fa9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "START_TOKEN 번호 : [10125]\n",
      "Q_TOKEN 번호 : [10126]\n",
      "A_TOKEN 번호 : [10127]\n",
      "END_TOKEN 번호 : [10128]\n",
      "10129\n"
     ]
    }
   ],
   "source": [
    "# 시작 토큰과 종료 토큰에 고유한 정수를 부여합니다.\n",
    "START_TOKEN, Q_TOKEN, A_TOKEN, END_TOKEN = [tokenizer.vocab_size], [tokenizer.vocab_size + 1], \\\n",
    "[tokenizer.vocab_size + 2], [tokenizer.vocab_size + 3]\n",
    "\n",
    "print('START_TOKEN 번호 :' ,[tokenizer.vocab_size])\n",
    "print('Q_TOKEN 번호 :' ,[tokenizer.vocab_size + 1])\n",
    "print('A_TOKEN 번호 :' ,[tokenizer.vocab_size + 2])\n",
    "print('END_TOKEN 번호 :' ,[tokenizer.vocab_size + 3])\n",
    "VOCAB_SIZE = tokenizer.vocab_size + 4\n",
    "print(VOCAB_SIZE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 348,
   "id": "7906f2a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def encode_sentence(sentence1, sentence2):\n",
    "    return START_TOKEN + Q_TOKEN + tokenizer.encode(sentence1)+ A_TOKEN + tokenizer.encode(sentence2)+ END_TOKEN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 349,
   "id": "148c2790",
   "metadata": {},
   "outputs": [],
   "source": [
    "encoded_sen = list(data.iloc[:, :2].apply(lambda x: encode_sentence(x[\"Q\"], x[\"A\"]), axis=1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 350,
   "id": "f639e167",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Q</th>\n",
       "      <th>A</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>12시 땡</td>\n",
       "      <td>하루가 또 가네요</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1지망 학교 떨어졌어</td>\n",
       "      <td>위로해 드립니다</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3박4일 놀러가고 싶다</td>\n",
       "      <td>여행은 언제나 좋죠</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3박4일 정도 놀러가고 싶다</td>\n",
       "      <td>여행은 언제나 좋죠</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>PPL 심하네</td>\n",
       "      <td>눈살이 찌푸려지죠</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11818</th>\n",
       "      <td>훔쳐보는 것도 눈치 보임</td>\n",
       "      <td>티가 나니까 눈치가 보이는 거죠</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11819</th>\n",
       "      <td>훔쳐보는 것도 눈치 보임</td>\n",
       "      <td>훔쳐보는 거 티나나봐요</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11820</th>\n",
       "      <td>흑기사 해주는 짝남</td>\n",
       "      <td>설렜겠어요</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11821</th>\n",
       "      <td>힘든 연애 좋은 연애라는게 무슨 차이일까</td>\n",
       "      <td>잘 헤어질 수 있는 사이 여부인 거 같아요</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11822</th>\n",
       "      <td>힘들어서 결혼할까봐</td>\n",
       "      <td>도피성 결혼은 하지 않길 바라요</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>11823 rows × 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                            Q                        A  label\n",
       "0                       12시 땡                하루가 또 가네요      0\n",
       "1                 1지망 학교 떨어졌어                 위로해 드립니다      0\n",
       "2                3박4일 놀러가고 싶다               여행은 언제나 좋죠      0\n",
       "3             3박4일 정도 놀러가고 싶다               여행은 언제나 좋죠      0\n",
       "4                     PPL 심하네                눈살이 찌푸려지죠      0\n",
       "...                       ...                      ...    ...\n",
       "11818           훔쳐보는 것도 눈치 보임        티가 나니까 눈치가 보이는 거죠      2\n",
       "11819           훔쳐보는 것도 눈치 보임             훔쳐보는 거 티나나봐요      2\n",
       "11820              흑기사 해주는 짝남                    설렜겠어요      2\n",
       "11821  힘든 연애 좋은 연애라는게 무슨 차이일까  잘 헤어질 수 있는 사이 여부인 거 같아요      2\n",
       "11822              힘들어서 결혼할까봐        도피성 결혼은 하지 않길 바라요      2\n",
       "\n",
       "[11823 rows x 3 columns]"
      ]
     },
     "execution_count": 350,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 351,
   "id": "854b5a90",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "원본 질문: Q            12시 땡\n",
      "A        하루가 또 가네요\n",
      "label            0\n",
      "Name: 0, dtype: object\n",
      "인코딩된 질문: [10125, 10126, 6932, 3006, 4776, 10127, 3324, 67, 6912, 10128]\n"
     ]
    }
   ],
   "source": [
    "# 10125 : START_TOKEN, 10126 : Q_TOKEN, 10127 : A_TOKEN, 10128 : END_TOKEN\n",
    "print(\"원본 질문:\", data.iloc[0])\n",
    "print(\"인코딩된 질문:\", encoded_sen[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 352,
   "id": "d511782b",
   "metadata": {},
   "outputs": [],
   "source": [
    "num_tokens = [len(tokens) for tokens in encoded_sen]\n",
    "num_tokens = np.array(num_tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 353,
   "id": "047a8159",
   "metadata": {},
   "outputs": [],
   "source": [
    "MAX_LENGTH = np.max(num_tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 354,
   "id": "9c371338",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 토큰화된 데이터에 패딩 추가\n",
    "tokenized_data = tf.keras.preprocessing.sequence.pad_sequences(\n",
    "      encoded_sen, maxlen=MAX_LENGTH, padding='post')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 355,
   "id": "dca6818a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(11823, 38)"
      ]
     },
     "execution_count": 355,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenized_data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 373,
   "id": "96e65da5",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "X = tokenized_data[:, :-1]  # 입력 시퀀스는 END_TOKEN 빠짐\n",
    "y = tokenized_data[:, 1:]   # 타겟 시퀀스는 START_TOKEN이 빠짐\n",
    "\n",
    "# 8대2로 나눔\n",
    "X_train, X_val, y_train, y_val = train_test_split(X, y, test_size=0.2, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 374,
   "id": "1aaf020f",
   "metadata": {},
   "outputs": [],
   "source": [
    "BATCH_SIZE = 64\n",
    "BUFFER_SIZE = 20000\n",
    "\n",
    "# 훈련 데이터셋\n",
    "train_dataset = tf.data.Dataset.from_tensor_slices(({'inputs': X_train}, {'outputs': y_train}))\n",
    "train_dataset = train_dataset.cache().shuffle(BUFFER_SIZE).batch(BATCH_SIZE).prefetch(tf.data.AUTOTUNE)\n",
    "\n",
    "# 검증 데이터셋\n",
    "val_dataset = tf.data.Dataset.from_tensor_slices(({'inputs': X_val}, {'outputs': y_val}))\n",
    "val_dataset = val_dataset.batch(BATCH_SIZE).prefetch(tf.data.AUTOTUNE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 375,
   "id": "6c8d900b",
   "metadata": {},
   "outputs": [],
   "source": [
    "NUM_LAYERS = 4 # 인코더와 디코더의 층의 개수\n",
    "D_MODEL = 128 # 인코더와 디코더 내부의 입, 출력의 고정 차원\n",
    "NUM_HEADS = 4 # 멀티 헤드 어텐션에서의 헤드 수\n",
    "UNITS = 256 # 피드 포워드 신경망의 은닉층의 크기\n",
    "DROPOUT = 0.1 # 드롭아웃의 비율\n",
    "\n",
    "model = gpt_decoder(\n",
    "    vocab_size=VOCAB_SIZE,\n",
    "    num_layers=NUM_LAYERS,\n",
    "    units=UNITS,\n",
    "    d_model=D_MODEL,\n",
    "    num_heads=NUM_HEADS,\n",
    "    max_position_encoding=max_len,\n",
    "    dropout=DROPOUT,\n",
    "    name='gpt_decoder_stack')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 376,
   "id": "9202a941",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"gpt_decoder_stack\"\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "inputs (InputLayer)             [(None, None)]       0                                            \n",
      "__________________________________________________________________________________________________\n",
      "look_ahead_mask (Lambda)        (None, 1, None, None 0           inputs[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "decoder (Functional)            (None, None, 128)    1831296     inputs[0][0]                     \n",
      "                                                                 look_ahead_mask[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "outputs (Dense)                 (None, None, 10129)  1306641     decoder[0][0]                    \n",
      "==================================================================================================\n",
      "Total params: 3,137,937\n",
      "Trainable params: 3,137,937\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "attachments": {
    "image.png": {
     "image/png": "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"
    }
   },
   "cell_type": "markdown",
   "id": "2be1816c",
   "metadata": {},
   "source": [
    "![image.png](attachment:image.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 377,
   "id": "36b84c6f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def loss_function(y_true, y_pred):\n",
    "    y_true = tf.reshape(y_true, shape=(-1, MAX_LENGTH - 1))\n",
    "\n",
    "    loss = tf.keras.losses.SparseCategoricalCrossentropy(\n",
    "      from_logits=True, reduction='none')(y_true, y_pred)\n",
    "\n",
    "    mask = tf.cast(tf.not_equal(y_true, 0), tf.float32)\n",
    "    loss = tf.multiply(loss, mask)\n",
    "\n",
    "    return tf.reduce_mean(loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 378,
   "id": "52a57e24",
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = tf.keras.optimizers.Adam(learning_rate=5e-5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 379,
   "id": "59545f5b",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(optimizer=optimizer, loss=loss_function)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 380,
   "id": "10f12346",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.callbacks import EarlyStopping\n",
    "early_stopping = EarlyStopping(\n",
    "    monitor='val_loss',        \n",
    "    patience=3,                # 3 에포크 동안 성능 향상이 없으면 학습 중단\n",
    "    restore_best_weights=True  # 가장 좋은 모델 가중치를 복원\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 381,
   "id": "28be4f84",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/200\n",
      "148/148 [==============================] - 11s 40ms/step - loss: 2.9610 - val_loss: 2.8701\n",
      "Epoch 2/200\n",
      "148/148 [==============================] - 5s 34ms/step - loss: 2.7548 - val_loss: 2.6933\n",
      "Epoch 3/200\n",
      "148/148 [==============================] - 5s 34ms/step - loss: 2.5896 - val_loss: 2.5474\n",
      "Epoch 4/200\n",
      "148/148 [==============================] - 5s 34ms/step - loss: 2.4547 - val_loss: 2.4304\n",
      "Epoch 5/200\n",
      "148/148 [==============================] - 5s 34ms/step - loss: 2.3485 - val_loss: 2.3425\n",
      "Epoch 6/200\n",
      "148/148 [==============================] - 5s 34ms/step - loss: 2.2696 - val_loss: 2.2790\n",
      "Epoch 7/200\n",
      "148/148 [==============================] - 5s 34ms/step - loss: 2.2105 - val_loss: 2.2302\n",
      "Epoch 8/200\n",
      "148/148 [==============================] - 5s 34ms/step - loss: 2.1607 - val_loss: 2.1866\n",
      "Epoch 9/200\n",
      "148/148 [==============================] - 5s 34ms/step - loss: 2.1158 - val_loss: 2.1530\n",
      "Epoch 10/200\n",
      "148/148 [==============================] - 5s 34ms/step - loss: 2.0800 - val_loss: 2.1247\n",
      "Epoch 11/200\n",
      "148/148 [==============================] - 5s 34ms/step - loss: 2.0507 - val_loss: 2.1033\n",
      "Epoch 12/200\n",
      "148/148 [==============================] - 5s 34ms/step - loss: 2.0257 - val_loss: 2.0850\n",
      "Epoch 13/200\n",
      "148/148 [==============================] - 5s 34ms/step - loss: 2.0038 - val_loss: 2.0702\n",
      "Epoch 14/200\n",
      "148/148 [==============================] - 5s 34ms/step - loss: 1.9843 - val_loss: 2.0565\n",
      "Epoch 15/200\n",
      "148/148 [==============================] - 5s 35ms/step - loss: 1.9671 - val_loss: 2.0438\n",
      "Epoch 16/200\n",
      "148/148 [==============================] - 5s 34ms/step - loss: 1.9509 - val_loss: 2.0338\n",
      "Epoch 17/200\n",
      "148/148 [==============================] - 5s 34ms/step - loss: 1.9356 - val_loss: 2.0251\n",
      "Epoch 18/200\n",
      "148/148 [==============================] - 5s 34ms/step - loss: 1.9215 - val_loss: 2.0144\n",
      "Epoch 19/200\n",
      "148/148 [==============================] - 5s 34ms/step - loss: 1.9076 - val_loss: 2.0047\n",
      "Epoch 20/200\n",
      "148/148 [==============================] - 5s 34ms/step - loss: 1.8943 - val_loss: 1.9977\n",
      "Epoch 21/200\n",
      "148/148 [==============================] - 5s 34ms/step - loss: 1.8816 - val_loss: 1.9904\n",
      "Epoch 22/200\n",
      "148/148 [==============================] - 5s 34ms/step - loss: 1.8690 - val_loss: 1.9818\n",
      "Epoch 23/200\n",
      "148/148 [==============================] - 5s 35ms/step - loss: 1.8560 - val_loss: 1.9754\n",
      "Epoch 24/200\n",
      "148/148 [==============================] - 5s 34ms/step - loss: 1.8439 - val_loss: 1.9687\n",
      "Epoch 25/200\n",
      "148/148 [==============================] - 5s 34ms/step - loss: 1.8321 - val_loss: 1.9616\n",
      "Epoch 26/200\n",
      "148/148 [==============================] - 5s 34ms/step - loss: 1.8202 - val_loss: 1.9566\n",
      "Epoch 27/200\n",
      "148/148 [==============================] - 5s 34ms/step - loss: 1.8082 - val_loss: 1.9505\n",
      "Epoch 28/200\n",
      "148/148 [==============================] - 5s 34ms/step - loss: 1.7967 - val_loss: 1.9447\n",
      "Epoch 29/200\n",
      "148/148 [==============================] - 5s 34ms/step - loss: 1.7850 - val_loss: 1.9393\n",
      "Epoch 30/200\n",
      "148/148 [==============================] - 5s 34ms/step - loss: 1.7739 - val_loss: 1.9342\n",
      "Epoch 31/200\n",
      "148/148 [==============================] - 5s 35ms/step - loss: 1.7630 - val_loss: 1.9308\n",
      "Epoch 32/200\n",
      "148/148 [==============================] - 5s 34ms/step - loss: 1.7520 - val_loss: 1.9246\n",
      "Epoch 33/200\n",
      "148/148 [==============================] - 5s 34ms/step - loss: 1.7415 - val_loss: 1.9213\n",
      "Epoch 34/200\n",
      "148/148 [==============================] - 5s 34ms/step - loss: 1.7307 - val_loss: 1.9160\n",
      "Epoch 35/200\n",
      "148/148 [==============================] - 5s 34ms/step - loss: 1.7205 - val_loss: 1.9133\n",
      "Epoch 36/200\n",
      "148/148 [==============================] - 5s 34ms/step - loss: 1.7099 - val_loss: 1.9091\n",
      "Epoch 37/200\n",
      "148/148 [==============================] - 5s 34ms/step - loss: 1.6994 - val_loss: 1.9043\n",
      "Epoch 38/200\n",
      "148/148 [==============================] - 5s 34ms/step - loss: 1.6891 - val_loss: 1.9034\n",
      "Epoch 39/200\n",
      "148/148 [==============================] - 5s 34ms/step - loss: 1.6794 - val_loss: 1.8988\n",
      "Epoch 40/200\n",
      "148/148 [==============================] - 5s 34ms/step - loss: 1.6694 - val_loss: 1.8959\n",
      "Epoch 41/200\n",
      "148/148 [==============================] - 5s 34ms/step - loss: 1.6595 - val_loss: 1.8926\n",
      "Epoch 42/200\n",
      "148/148 [==============================] - 5s 34ms/step - loss: 1.6496 - val_loss: 1.8918\n",
      "Epoch 43/200\n",
      "148/148 [==============================] - 5s 34ms/step - loss: 1.6400 - val_loss: 1.8883\n",
      "Epoch 44/200\n",
      "148/148 [==============================] - 5s 34ms/step - loss: 1.6299 - val_loss: 1.8844\n",
      "Epoch 45/200\n",
      "148/148 [==============================] - 5s 34ms/step - loss: 1.6201 - val_loss: 1.8837\n",
      "Epoch 46/200\n",
      "148/148 [==============================] - 5s 35ms/step - loss: 1.6101 - val_loss: 1.8810\n",
      "Epoch 47/200\n",
      "148/148 [==============================] - 5s 35ms/step - loss: 1.6003 - val_loss: 1.8805\n",
      "Epoch 48/200\n",
      "148/148 [==============================] - 5s 35ms/step - loss: 1.5905 - val_loss: 1.8745\n",
      "Epoch 49/200\n",
      "148/148 [==============================] - 5s 35ms/step - loss: 1.5806 - val_loss: 1.8742\n",
      "Epoch 50/200\n",
      "148/148 [==============================] - 5s 34ms/step - loss: 1.5716 - val_loss: 1.8722\n",
      "Epoch 51/200\n",
      "148/148 [==============================] - 5s 34ms/step - loss: 1.5612 - val_loss: 1.8707\n",
      "Epoch 52/200\n",
      "148/148 [==============================] - 5s 34ms/step - loss: 1.5517 - val_loss: 1.8678\n",
      "Epoch 53/200\n",
      "148/148 [==============================] - 5s 34ms/step - loss: 1.5417 - val_loss: 1.8660\n",
      "Epoch 54/200\n",
      "148/148 [==============================] - 5s 34ms/step - loss: 1.5327 - val_loss: 1.8658\n",
      "Epoch 55/200\n",
      "148/148 [==============================] - 5s 34ms/step - loss: 1.5231 - val_loss: 1.8646\n",
      "Epoch 56/200\n",
      "148/148 [==============================] - 5s 34ms/step - loss: 1.5138 - val_loss: 1.8621\n",
      "Epoch 57/200\n",
      "148/148 [==============================] - 5s 34ms/step - loss: 1.5039 - val_loss: 1.8601\n",
      "Epoch 58/200\n",
      "148/148 [==============================] - 5s 34ms/step - loss: 1.4939 - val_loss: 1.8597\n",
      "Epoch 59/200\n",
      "148/148 [==============================] - 5s 34ms/step - loss: 1.4842 - val_loss: 1.8581\n",
      "Epoch 60/200\n",
      "148/148 [==============================] - 5s 34ms/step - loss: 1.4748 - val_loss: 1.8580\n",
      "Epoch 61/200\n",
      "148/148 [==============================] - 5s 34ms/step - loss: 1.4655 - val_loss: 1.8535\n",
      "Epoch 62/200\n",
      "148/148 [==============================] - 5s 34ms/step - loss: 1.4552 - val_loss: 1.8552\n",
      "Epoch 63/200\n",
      "148/148 [==============================] - 5s 34ms/step - loss: 1.4457 - val_loss: 1.8543\n",
      "Epoch 64/200\n",
      "148/148 [==============================] - 5s 34ms/step - loss: 1.4360 - val_loss: 1.8513\n",
      "Epoch 65/200\n",
      "148/148 [==============================] - 5s 34ms/step - loss: 1.4266 - val_loss: 1.8517\n",
      "Epoch 66/200\n",
      "148/148 [==============================] - 5s 34ms/step - loss: 1.4167 - val_loss: 1.8502\n",
      "Epoch 67/200\n",
      "148/148 [==============================] - 5s 34ms/step - loss: 1.4082 - val_loss: 1.8480\n",
      "Epoch 68/200\n",
      "148/148 [==============================] - 5s 34ms/step - loss: 1.3986 - val_loss: 1.8484\n",
      "Epoch 69/200\n",
      "148/148 [==============================] - 5s 34ms/step - loss: 1.3887 - val_loss: 1.8476\n",
      "Epoch 70/200\n",
      "148/148 [==============================] - 5s 34ms/step - loss: 1.3785 - val_loss: 1.8457\n",
      "Epoch 71/200\n",
      "148/148 [==============================] - 5s 34ms/step - loss: 1.3691 - val_loss: 1.8452\n",
      "Epoch 72/200\n",
      "148/148 [==============================] - 5s 34ms/step - loss: 1.3601 - val_loss: 1.8433\n",
      "Epoch 73/200\n",
      "148/148 [==============================] - 5s 34ms/step - loss: 1.3500 - val_loss: 1.8437\n",
      "Epoch 74/200\n",
      "148/148 [==============================] - 5s 35ms/step - loss: 1.3409 - val_loss: 1.8439\n",
      "Epoch 75/200\n",
      "148/148 [==============================] - 5s 35ms/step - loss: 1.3313 - val_loss: 1.8424\n",
      "Epoch 76/200\n",
      "148/148 [==============================] - 5s 34ms/step - loss: 1.3208 - val_loss: 1.8434\n",
      "Epoch 77/200\n",
      "148/148 [==============================] - 5s 35ms/step - loss: 1.3126 - val_loss: 1.8416\n",
      "Epoch 78/200\n",
      "148/148 [==============================] - 5s 35ms/step - loss: 1.3032 - val_loss: 1.8405\n",
      "Epoch 79/200\n",
      "148/148 [==============================] - 5s 34ms/step - loss: 1.2938 - val_loss: 1.8394\n",
      "Epoch 80/200\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "148/148 [==============================] - 5s 34ms/step - loss: 1.2835 - val_loss: 1.8398\n",
      "Epoch 81/200\n",
      "148/148 [==============================] - 5s 34ms/step - loss: 1.2751 - val_loss: 1.8399\n",
      "Epoch 82/200\n",
      "148/148 [==============================] - 5s 34ms/step - loss: 1.2648 - val_loss: 1.8404\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x763635045910>"
      ]
     },
     "execution_count": 381,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "EPOCHS = 200\n",
    "model.fit(train_dataset, validation_data = val_dataset, epochs=EPOCHS, callbacks = [early_stopping], verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 405,
   "id": "bfec1582",
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_answer(question, tokenizer, model, max_length=MAX_LENGTH):\n",
    "    # 1. 질문을 토큰화하여 입력 텍스트 준비\n",
    "    question_encoded = tokenizer.encode(question)\n",
    "    input_ids = [tokenizer.vocab_size + 1] + question_encoded  # <START_TOKEN> 추가\n",
    "\n",
    "    # 2. 텍스트를 텐서로 변환\n",
    "    input_tensor = tf.constant(input_ids, dtype=tf.int64)  # 입력 시퀀스를 텐서로 변환\n",
    "    input_tensor = tf.expand_dims(input_tensor, 0)  # 배치 차원 추가 (배치 크기 1)\n",
    "\n",
    "    # 3. 예측 시작\n",
    "    generated_ids = []  # 시작 토큰을 제외한 예측된 토큰들을 저장할 리스트\n",
    "    predicted_answer = []\n",
    "    for _ in range(max_length - len(input_ids)):  # 최대 길이까지 예측\n",
    "        # 모델 예측 (자기회귀적 예측)\n",
    "        predictions = model(input_tensor, training=False)  # 모델 예측 (디코더 입력)\n",
    "\n",
    "        # 예측된 값 중 가장 확률이 높은 토큰을 선택\n",
    "        logits = predictions[:, -1, :]  # 마지막 타임스텝의 로짓\n",
    "        predicted_token = tf.argmax(logits, axis=-1)  # 가장 확률이 높은 토큰 선택\n",
    "        predicted_token = tf.cast(predicted_token, tf.int64)  # 정수로 형변환\n",
    "        \n",
    "        # 종료 토큰 <END_TOKEN>이 나오면 예측을 멈춘다\n",
    "        if predicted_token == tokenizer.vocab_size + 4:  # <END_TOKEN> (vocab_size + 4)\n",
    "            print('end_token 등장')\n",
    "            break\n",
    "\n",
    "        # 예측된 토큰을 입력 시퀀스에 추가\n",
    "        input_tensor = tf.concat([input_tensor, tf.expand_dims(predicted_token, 0)], axis=-1)\n",
    "\n",
    "        # 생성된 ID를 저장\n",
    "        generated_ids.append(predicted_token.numpy()[0])  # 시작 토큰은 제외하고 저장\n",
    "        \n",
    "        temp = predicted_token.numpy()[0]\n",
    "#         print(temp)\n",
    "        if temp == 10125:\n",
    "            decode_answer = 'start'\n",
    "        elif temp == 10126:\n",
    "            decode_answer = 'q'\n",
    "        elif temp == 10127:\n",
    "            decode_answer = 'a'\n",
    "        elif temp == 10128:\n",
    "            decode_answer = 'end'\n",
    "        else:\n",
    "            decode_answer = tokenizer.decode([temp])\n",
    "        predicted_answer.append(decode_answer)\n",
    "        \n",
    "    # 4. 예측된 토큰을 디코딩하여 답변 생성\n",
    "    print(generated_ids)\n",
    "#     predicted_answer = tokenizer.decode(generated_ids)  # 시작 토큰 제외하고 디코딩\n",
    "    return predicted_answer\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 411,
   "id": "11c0b5e4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[10127, 25, 232, 2, 22, 1, 10128, 10128, 10128, 10128, 10128, 10128, 10128, 10128, 10128, 10128, 10128, 10128, 10128, 10128, 10128, 10128, 10128, 10128, 10128, 10128, 10128, 10128, 10128, 10128, 10128, 10128, 10128, 10128]\n",
      "Generated Answer: a 사람  만날  수  있을  거예요 end end end end end end end end end end end end end end end end end end end end end end end end end end end end\n"
     ]
    }
   ],
   "source": [
    "question = \"결혼이 뭐야\"\n",
    "\n",
    "# 답변 생성\n",
    "answer = predict_answer(question, tokenizer, model)\n",
    "print(\"Generated Answer:\", ' '.join(answer))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f516c6eb",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b1debaa3",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "68582970",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20c28f97",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b0461cd9",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7396894f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cfc4a797",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "45183cef",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf404997",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b7d1780",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "385fd347",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2991d6c4",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c1958968",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "67e3d6e7",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a2e7370",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f5e0d306",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba8065be",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0820142e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "708e6e11",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a0cb480d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a76f386",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
